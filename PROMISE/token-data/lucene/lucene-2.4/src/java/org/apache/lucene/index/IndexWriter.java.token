package
org
.
apache
.
lucene
.
index
;
import
org
.
apache
.
lucene
.
analysis
.
Analyzer
;
import
org
.
apache
.
lucene
.
document
.
Document
;
import
org
.
apache
.
lucene
.
search
.
Similarity
;
import
org
.
apache
.
lucene
.
search
.
Query
;
import
org
.
apache
.
lucene
.
store
.
Directory
;
import
org
.
apache
.
lucene
.
store
.
FSDirectory
;
import
org
.
apache
.
lucene
.
store
.
Lock
;
import
org
.
apache
.
lucene
.
store
.
LockObtainFailedException
;
import
org
.
apache
.
lucene
.
store
.
AlreadyClosedException
;
import
org
.
apache
.
lucene
.
util
.
BitVector
;
import
org
.
apache
.
lucene
.
util
.
Constants
;
import
java
.
io
.
File
;
import
java
.
io
.
IOException
;
import
java
.
io
.
PrintStream
;
import
java
.
util
.
List
;
import
java
.
util
.
Collection
;
import
java
.
util
.
ArrayList
;
import
java
.
util
.
HashMap
;
import
java
.
util
.
Set
;
import
java
.
util
.
HashSet
;
import
java
.
util
.
LinkedList
;
import
java
.
util
.
Iterator
;
public
class
IndexWriter
{
public
static
long
WRITE_LOCK_TIMEOUT
=
1000
;
private
long
writeLockTimeout
=
WRITE_LOCK_TIMEOUT
;
public
static
final
String
WRITE_LOCK_NAME
=
"write.lock"
;
public
final
static
int
DEFAULT_MERGE_FACTOR
=
LogMergePolicy
.
DEFAULT_MERGE_FACTOR
;
public
final
static
int
DISABLE_AUTO_FLUSH
=
-
1
;
public
final
static
int
DEFAULT_MAX_BUFFERED_DOCS
=
DISABLE_AUTO_FLUSH
;
public
final
static
double
DEFAULT_RAM_BUFFER_SIZE_MB
=
16.0
;
public
final
static
int
DEFAULT_MAX_BUFFERED_DELETE_TERMS
=
DISABLE_AUTO_FLUSH
;
public
final
static
int
DEFAULT_MAX_MERGE_DOCS
=
LogDocMergePolicy
.
DEFAULT_MAX_MERGE_DOCS
;
public
final
static
int
DEFAULT_MAX_FIELD_LENGTH
=
10000
;
public
final
static
int
DEFAULT_TERM_INDEX_INTERVAL
=
128
;
public
final
static
int
MAX_TERM_LENGTH
=
DocumentsWriter
.
MAX_TERM_LENGTH
;
public
final
static
double
DEFAULT_MAX_SYNC_PAUSE_SECONDS
;
static
{
if
(
Constants
.
WINDOWS
)
DEFAULT_MAX_SYNC_PAUSE_SECONDS
=
10.0
;
else
DEFAULT_MAX_SYNC_PAUSE_SECONDS
=
0.0
;
}
private
final
static
int
MERGE_READ_BUFFER_SIZE
=
4096
;
private
static
Object
MESSAGE_ID_LOCK
=
new
Object
(
)
;
private
static
int
MESSAGE_ID
=
0
;
private
int
messageID
=
-
1
;
volatile
private
boolean
hitOOM
;
private
Directory
directory
;
private
Analyzer
analyzer
;
private
Similarity
similarity
=
Similarity
.
getDefault
(
)
;
private
volatile
long
changeCount
;
private
long
lastCommitChangeCount
;
private
SegmentInfos
rollbackSegmentInfos
;
private
HashMap
rollbackSegments
;
volatile
SegmentInfos
pendingCommit
;
volatile
long
pendingCommitChangeCount
;
private
SegmentInfos
localRollbackSegmentInfos
;
private
boolean
localAutoCommit
;
private
int
localFlushedDocCount
;
private
boolean
autoCommit
=
true
;
private
SegmentInfos
segmentInfos
=
new
SegmentInfos
(
)
;
private
DocumentsWriter
docWriter
;
private
IndexFileDeleter
deleter
;
private
Set
segmentsToOptimize
=
new
HashSet
(
)
;
private
Lock
writeLock
;
private
int
termIndexInterval
=
DEFAULT_TERM_INDEX_INTERVAL
;
private
boolean
closeDir
;
private
boolean
closed
;
private
boolean
closing
;
private
HashSet
mergingSegments
=
new
HashSet
(
)
;
private
MergePolicy
mergePolicy
=
new
LogByteSizeMergePolicy
(
)
;
private
MergeScheduler
mergeScheduler
=
new
ConcurrentMergeScheduler
(
)
;
private
LinkedList
pendingMerges
=
new
LinkedList
(
)
;
private
Set
runningMerges
=
new
HashSet
(
)
;
private
List
mergeExceptions
=
new
ArrayList
(
)
;
private
long
mergeGen
;
private
boolean
stopMerges
;
private
int
flushCount
;
private
int
flushDeletesCount
;
private
double
maxSyncPauseSeconds
=
DEFAULT_MAX_SYNC_PAUSE_SECONDS
;
private
int
readCount
;
private
Thread
writeThread
;
synchronized
void
acquireWrite
(
)
{
while
(
writeThread
!=
null
||
readCount
>
0
)
doWait
(
)
;
ensureOpen
(
)
;
writeThread
=
Thread
.
currentThread
(
)
;
}
synchronized
void
releaseWrite
(
)
{
assert
Thread
.
currentThread
(
)
==
writeThread
;
writeThread
=
null
;
notifyAll
(
)
;
}
synchronized
void
acquireRead
(
)
{
final
Thread
current
=
Thread
.
currentThread
(
)
;
while
(
writeThread
!=
null
&&
writeThread
!=
current
)
doWait
(
)
;
readCount
++
;
}
synchronized
void
releaseRead
(
)
{
readCount
--
;
assert
readCount
>=
0
;
if
(
0
==
readCount
)
notifyAll
(
)
;
}
protected
synchronized
final
void
ensureOpen
(
boolean
includePendingClose
)
throws
AlreadyClosedException
{
if
(
closed
||
(
includePendingClose
&&
closing
)
)
{
throw
new
AlreadyClosedException
(
"this IndexWriter is closed"
)
;
}
}
protected
synchronized
final
void
ensureOpen
(
)
throws
AlreadyClosedException
{
ensureOpen
(
true
)
;
}
public
void
message
(
String
message
)
{
if
(
infoStream
!=
null
)
infoStream
.
println
(
"IW "
+
messageID
+
" ["
+
Thread
.
currentThread
(
)
.
getName
(
)
+
"]: "
+
message
)
;
}
private
synchronized
void
setMessageID
(
PrintStream
infoStream
)
{
if
(
infoStream
!=
null
&&
messageID
==
-
1
)
{
synchronized
(
MESSAGE_ID_LOCK
)
{
messageID
=
MESSAGE_ID
++
;
}
}
this
.
infoStream
=
infoStream
;
}
private
LogMergePolicy
getLogMergePolicy
(
)
{
if
(
mergePolicy
instanceof
LogMergePolicy
)
return
(
LogMergePolicy
)
mergePolicy
;
else
throw
new
IllegalArgumentException
(
"this method can only be called when the merge policy is the default LogMergePolicy"
)
;
}
public
boolean
getUseCompoundFile
(
)
{
return
getLogMergePolicy
(
)
.
getUseCompoundFile
(
)
;
}
public
void
setUseCompoundFile
(
boolean
value
)
{
getLogMergePolicy
(
)
.
setUseCompoundFile
(
value
)
;
getLogMergePolicy
(
)
.
setUseCompoundDocStore
(
value
)
;
}
public
void
setSimilarity
(
Similarity
similarity
)
{
ensureOpen
(
)
;
this
.
similarity
=
similarity
;
docWriter
.
setSimilarity
(
similarity
)
;
}
public
Similarity
getSimilarity
(
)
{
ensureOpen
(
)
;
return
this
.
similarity
;
}
public
void
setTermIndexInterval
(
int
interval
)
{
ensureOpen
(
)
;
this
.
termIndexInterval
=
interval
;
}
public
int
getTermIndexInterval
(
)
{
ensureOpen
(
false
)
;
return
termIndexInterval
;
}
public
IndexWriter
(
String
path
,
Analyzer
a
,
boolean
create
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
create
,
true
,
null
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
String
path
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
create
,
true
,
null
,
true
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
File
path
,
Analyzer
a
,
boolean
create
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
create
,
true
,
null
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
File
path
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
create
,
true
,
null
,
true
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
,
boolean
create
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
null
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
null
,
true
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
String
path
,
Analyzer
a
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
true
,
null
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
String
path
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
true
,
null
,
true
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
File
path
,
Analyzer
a
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
true
,
null
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
File
path
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
true
,
null
,
true
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
null
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
null
,
true
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
null
,
autoCommit
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
null
,
autoCommit
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
,
IndexDeletionPolicy
deletionPolicy
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
deletionPolicy
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
,
IndexDeletionPolicy
deletionPolicy
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
deletionPolicy
,
autoCommit
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
,
boolean
create
,
IndexDeletionPolicy
deletionPolicy
,
MaxFieldLength
mfl
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
deletionPolicy
,
false
,
mfl
.
getLimit
(
)
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
,
boolean
create
,
IndexDeletionPolicy
deletionPolicy
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
deletionPolicy
,
autoCommit
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
private
void
init
(
Directory
d
,
Analyzer
a
,
boolean
closeDir
,
IndexDeletionPolicy
deletionPolicy
,
boolean
autoCommit
,
int
maxFieldLength
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
if
(
IndexReader
.
indexExists
(
d
)
)
{
init
(
d
,
a
,
false
,
closeDir
,
deletionPolicy
,
autoCommit
,
maxFieldLength
)
;
}
else
{
init
(
d
,
a
,
true
,
closeDir
,
deletionPolicy
,
autoCommit
,
maxFieldLength
)
;
}
}
private
void
init
(
Directory
d
,
Analyzer
a
,
final
boolean
create
,
boolean
closeDir
,
IndexDeletionPolicy
deletionPolicy
,
boolean
autoCommit
,
int
maxFieldLength
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
this
.
closeDir
=
closeDir
;
directory
=
d
;
analyzer
=
a
;
setMessageID
(
defaultInfoStream
)
;
this
.
maxFieldLength
=
maxFieldLength
;
if
(
create
)
{
directory
.
clearLock
(
WRITE_LOCK_NAME
)
;
}
Lock
writeLock
=
directory
.
makeLock
(
WRITE_LOCK_NAME
)
;
if
(
!
writeLock
.
obtain
(
writeLockTimeout
)
)
throw
new
LockObtainFailedException
(
"Index locked for write: "
+
writeLock
)
;
this
.
writeLock
=
writeLock
;
try
{
if
(
create
)
{
try
{
segmentInfos
.
read
(
directory
)
;
segmentInfos
.
clear
(
)
;
}
catch
(
IOException
e
)
{
}
segmentInfos
.
commit
(
directory
)
;
}
else
{
segmentInfos
.
read
(
directory
)
;
for
(
int
i
=
0
;
i
<
segmentInfos
.
size
(
)
;
i
++
)
{
final
SegmentInfo
info
=
segmentInfos
.
info
(
i
)
;
List
files
=
info
.
files
(
)
;
for
(
int
j
=
0
;
j
<
files
.
size
(
)
;
j
++
)
synced
.
add
(
files
.
get
(
j
)
)
;
}
}
this
.
autoCommit
=
autoCommit
;
setRollbackSegmentInfos
(
segmentInfos
)
;
docWriter
=
new
DocumentsWriter
(
directory
,
this
)
;
docWriter
.
setInfoStream
(
infoStream
)
;
docWriter
.
setMaxFieldLength
(
maxFieldLength
)
;
deleter
=
new
IndexFileDeleter
(
directory
,
deletionPolicy
==
null
?
new
KeepOnlyLastCommitDeletionPolicy
(
)
:
deletionPolicy
,
segmentInfos
,
infoStream
,
docWriter
)
;
pushMaxBufferedDocs
(
)
;
if
(
infoStream
!=
null
)
{
message
(
"init: create="
+
create
)
;
messageState
(
)
;
}
}
catch
(
IOException
e
)
{
this
.
writeLock
.
release
(
)
;
this
.
writeLock
=
null
;
throw
e
;
}
}
private
synchronized
void
setRollbackSegmentInfos
(
SegmentInfos
infos
)
{
rollbackSegmentInfos
=
(
SegmentInfos
)
infos
.
clone
(
)
;
assert
!
hasExternalSegments
(
rollbackSegmentInfos
)
;
rollbackSegments
=
new
HashMap
(
)
;
final
int
size
=
rollbackSegmentInfos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
size
;
i
++
)
rollbackSegments
.
put
(
rollbackSegmentInfos
.
info
(
i
)
,
new
Integer
(
i
)
)
;
}
public
void
setMergePolicy
(
MergePolicy
mp
)
{
ensureOpen
(
)
;
if
(
mp
==
null
)
throw
new
NullPointerException
(
"MergePolicy must be non-null"
)
;
if
(
mergePolicy
!=
mp
)
mergePolicy
.
close
(
)
;
mergePolicy
=
mp
;
pushMaxBufferedDocs
(
)
;
if
(
infoStream
!=
null
)
message
(
"setMergePolicy "
+
mp
)
;
}
public
MergePolicy
getMergePolicy
(
)
{
ensureOpen
(
)
;
return
mergePolicy
;
}
synchronized
public
void
setMergeScheduler
(
MergeScheduler
mergeScheduler
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
if
(
mergeScheduler
==
null
)
throw
new
NullPointerException
(
"MergeScheduler must be non-null"
)
;
if
(
this
.
mergeScheduler
!=
mergeScheduler
)
{
finishMerges
(
true
)
;
this
.
mergeScheduler
.
close
(
)
;
}
this
.
mergeScheduler
=
mergeScheduler
;
if
(
infoStream
!=
null
)
message
(
"setMergeScheduler "
+
mergeScheduler
)
;
}
public
MergeScheduler
getMergeScheduler
(
)
{
ensureOpen
(
)
;
return
mergeScheduler
;
}
public
void
setMaxMergeDocs
(
int
maxMergeDocs
)
{
getLogMergePolicy
(
)
.
setMaxMergeDocs
(
maxMergeDocs
)
;
}
public
int
getMaxMergeDocs
(
)
{
return
getLogMergePolicy
(
)
.
getMaxMergeDocs
(
)
;
}
public
void
setMaxFieldLength
(
int
maxFieldLength
)
{
ensureOpen
(
)
;
this
.
maxFieldLength
=
maxFieldLength
;
docWriter
.
setMaxFieldLength
(
maxFieldLength
)
;
if
(
infoStream
!=
null
)
message
(
"setMaxFieldLength "
+
maxFieldLength
)
;
}
public
int
getMaxFieldLength
(
)
{
ensureOpen
(
)
;
return
maxFieldLength
;
}
public
void
setMaxBufferedDocs
(
int
maxBufferedDocs
)
{
ensureOpen
(
)
;
if
(
maxBufferedDocs
!=
DISABLE_AUTO_FLUSH
&&
maxBufferedDocs
<
2
)
throw
new
IllegalArgumentException
(
"maxBufferedDocs must at least be 2 when enabled"
)
;
if
(
maxBufferedDocs
==
DISABLE_AUTO_FLUSH
&&
getRAMBufferSizeMB
(
)
==
DISABLE_AUTO_FLUSH
)
throw
new
IllegalArgumentException
(
"at least one of ramBufferSize and maxBufferedDocs must be enabled"
)
;
docWriter
.
setMaxBufferedDocs
(
maxBufferedDocs
)
;
pushMaxBufferedDocs
(
)
;
if
(
infoStream
!=
null
)
message
(
"setMaxBufferedDocs "
+
maxBufferedDocs
)
;
}
private
void
pushMaxBufferedDocs
(
)
{
if
(
docWriter
.
getMaxBufferedDocs
(
)
!=
DISABLE_AUTO_FLUSH
)
{
final
MergePolicy
mp
=
mergePolicy
;
if
(
mp
instanceof
LogDocMergePolicy
)
{
LogDocMergePolicy
lmp
=
(
LogDocMergePolicy
)
mp
;
final
int
maxBufferedDocs
=
docWriter
.
getMaxBufferedDocs
(
)
;
if
(
lmp
.
getMinMergeDocs
(
)
!=
maxBufferedDocs
)
{
if
(
infoStream
!=
null
)
message
(
"now push maxBufferedDocs "
+
maxBufferedDocs
+
" to LogDocMergePolicy"
)
;
lmp
.
setMinMergeDocs
(
maxBufferedDocs
)
;
}
}
}
}
public
int
getMaxBufferedDocs
(
)
{
ensureOpen
(
)
;
return
docWriter
.
getMaxBufferedDocs
(
)
;
}
public
void
setRAMBufferSizeMB
(
double
mb
)
{
if
(
mb
!=
DISABLE_AUTO_FLUSH
&&
mb
<=
0.0
)
throw
new
IllegalArgumentException
(
"ramBufferSize should be > 0.0 MB when enabled"
)
;
if
(
mb
==
DISABLE_AUTO_FLUSH
&&
getMaxBufferedDocs
(
)
==
DISABLE_AUTO_FLUSH
)
throw
new
IllegalArgumentException
(
"at least one of ramBufferSize and maxBufferedDocs must be enabled"
)
;
docWriter
.
setRAMBufferSizeMB
(
mb
)
;
if
(
infoStream
!=
null
)
message
(
"setRAMBufferSizeMB "
+
mb
)
;
}
public
double
getRAMBufferSizeMB
(
)
{
return
docWriter
.
getRAMBufferSizeMB
(
)
;
}
public
void
setMaxBufferedDeleteTerms
(
int
maxBufferedDeleteTerms
)
{
ensureOpen
(
)
;
if
(
maxBufferedDeleteTerms
!=
DISABLE_AUTO_FLUSH
&&
maxBufferedDeleteTerms
<
1
)
throw
new
IllegalArgumentException
(
"maxBufferedDeleteTerms must at least be 1 when enabled"
)
;
docWriter
.
setMaxBufferedDeleteTerms
(
maxBufferedDeleteTerms
)
;
if
(
infoStream
!=
null
)
message
(
"setMaxBufferedDeleteTerms "
+
maxBufferedDeleteTerms
)
;
}
public
int
getMaxBufferedDeleteTerms
(
)
{
ensureOpen
(
)
;
return
docWriter
.
getMaxBufferedDeleteTerms
(
)
;
}
public
void
setMergeFactor
(
int
mergeFactor
)
{
getLogMergePolicy
(
)
.
setMergeFactor
(
mergeFactor
)
;
}
public
int
getMergeFactor
(
)
{
return
getLogMergePolicy
(
)
.
getMergeFactor
(
)
;
}
public
double
getMaxSyncPauseSeconds
(
)
{
return
maxSyncPauseSeconds
;
}
public
void
setMaxSyncPauseSeconds
(
double
seconds
)
{
maxSyncPauseSeconds
=
seconds
;
}
public
static
void
setDefaultInfoStream
(
PrintStream
infoStream
)
{
IndexWriter
.
defaultInfoStream
=
infoStream
;
}
public
static
PrintStream
getDefaultInfoStream
(
)
{
return
IndexWriter
.
defaultInfoStream
;
}
public
void
setInfoStream
(
PrintStream
infoStream
)
{
ensureOpen
(
)
;
setMessageID
(
infoStream
)
;
docWriter
.
setInfoStream
(
infoStream
)
;
deleter
.
setInfoStream
(
infoStream
)
;
if
(
infoStream
!=
null
)
messageState
(
)
;
}
private
void
messageState
(
)
{
message
(
"setInfoStream: dir="
+
directory
+
" autoCommit="
+
autoCommit
+
" mergePolicy="
+
mergePolicy
+
" mergeScheduler="
+
mergeScheduler
+
" ramBufferSizeMB="
+
docWriter
.
getRAMBufferSizeMB
(
)
+
" maxBufferedDocs="
+
docWriter
.
getMaxBufferedDocs
(
)
+
" maxBuffereDeleteTerms="
+
docWriter
.
getMaxBufferedDeleteTerms
(
)
+
" maxFieldLength="
+
maxFieldLength
+
" index="
+
segString
(
)
)
;
}
public
PrintStream
getInfoStream
(
)
{
ensureOpen
(
)
;
return
infoStream
;
}
public
void
setWriteLockTimeout
(
long
writeLockTimeout
)
{
ensureOpen
(
)
;
this
.
writeLockTimeout
=
writeLockTimeout
;
}
public
long
getWriteLockTimeout
(
)
{
ensureOpen
(
)
;
return
writeLockTimeout
;
}
public
static
void
setDefaultWriteLockTimeout
(
long
writeLockTimeout
)
{
IndexWriter
.
WRITE_LOCK_TIMEOUT
=
writeLockTimeout
;
}
public
static
long
getDefaultWriteLockTimeout
(
)
{
return
IndexWriter
.
WRITE_LOCK_TIMEOUT
;
}
public
void
close
(
)
throws
CorruptIndexException
,
IOException
{
close
(
true
)
;
}
public
void
close
(
boolean
waitForMerges
)
throws
CorruptIndexException
,
IOException
{
if
(
hitOOM
)
{
rollback
(
)
;
return
;
}
if
(
shouldClose
(
)
)
closeInternal
(
waitForMerges
)
;
}
synchronized
private
boolean
shouldClose
(
)
{
while
(
true
)
{
if
(
!
closed
)
{
if
(
!
closing
)
{
closing
=
true
;
return
true
;
}
else
{
doWait
(
)
;
}
}
else
return
false
;
}
}
private
void
closeInternal
(
boolean
waitForMerges
)
throws
CorruptIndexException
,
IOException
{
docWriter
.
pauseAllThreads
(
)
;
try
{
if
(
infoStream
!=
null
)
message
(
"now flush at close"
)
;
docWriter
.
close
(
)
;
flush
(
waitForMerges
,
true
,
true
)
;
if
(
waitForMerges
)
mergeScheduler
.
merge
(
this
)
;
mergePolicy
.
close
(
)
;
finishMerges
(
waitForMerges
)
;
mergeScheduler
.
close
(
)
;
if
(
infoStream
!=
null
)
message
(
"now call final commit()"
)
;
commit
(
0
)
;
if
(
infoStream
!=
null
)
message
(
"at close: "
+
segString
(
)
)
;
synchronized
(
this
)
{
docWriter
=
null
;
deleter
.
close
(
)
;
}
if
(
closeDir
)
directory
.
close
(
)
;
if
(
writeLock
!=
null
)
{
writeLock
.
release
(
)
;
writeLock
=
null
;
}
synchronized
(
this
)
{
closed
=
true
;
}
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
finally
{
synchronized
(
this
)
{
closing
=
false
;
notifyAll
(
)
;
if
(
!
closed
)
{
if
(
docWriter
!=
null
)
docWriter
.
resumeAllThreads
(
)
;
if
(
infoStream
!=
null
)
message
(
"hit exception while closing"
)
;
}
}
}
}
private
synchronized
boolean
flushDocStores
(
)
throws
IOException
{
boolean
useCompoundDocStore
=
false
;
String
docStoreSegment
;
boolean
success
=
false
;
try
{
docStoreSegment
=
docWriter
.
closeDocStore
(
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception closing doc store segment"
)
;
}
}
useCompoundDocStore
=
mergePolicy
.
useCompoundDocStore
(
segmentInfos
)
;
if
(
useCompoundDocStore
&&
docStoreSegment
!=
null
&&
docWriter
.
closedFiles
(
)
.
size
(
)
!=
0
)
{
success
=
false
;
final
int
numSegments
=
segmentInfos
.
size
(
)
;
final
String
compoundFileName
=
docStoreSegment
+
"."
+
IndexFileNames
.
COMPOUND_FILE_STORE_EXTENSION
;
try
{
CompoundFileWriter
cfsWriter
=
new
CompoundFileWriter
(
directory
,
compoundFileName
)
;
final
Iterator
it
=
docWriter
.
closedFiles
(
)
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
cfsWriter
.
addFile
(
(
String
)
it
.
next
(
)
)
;
cfsWriter
.
close
(
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception building compound file doc store for segment "
+
docStoreSegment
)
;
deleter
.
deleteFile
(
compoundFileName
)
;
}
}
for
(
int
i
=
0
;
i
<
numSegments
;
i
++
)
{
SegmentInfo
si
=
segmentInfos
.
info
(
i
)
;
if
(
si
.
getDocStoreOffset
(
)
!=
-
1
&&
si
.
getDocStoreSegment
(
)
.
equals
(
docStoreSegment
)
)
si
.
setDocStoreIsCompoundFile
(
true
)
;
}
checkpoint
(
)
;
deleter
.
deleteNewFiles
(
docWriter
.
closedFiles
(
)
)
;
}
return
useCompoundDocStore
;
}
protected
void
finalize
(
)
throws
Throwable
{
try
{
if
(
writeLock
!=
null
)
{
writeLock
.
release
(
)
;
writeLock
=
null
;
}
}
finally
{
super
.
finalize
(
)
;
}
}
public
Directory
getDirectory
(
)
{
ensureOpen
(
false
)
;
return
directory
;
}
public
Analyzer
getAnalyzer
(
)
{
ensureOpen
(
)
;
return
analyzer
;
}
public
synchronized
int
docCount
(
)
{
ensureOpen
(
)
;
return
maxDoc
(
)
;
}
public
synchronized
int
maxDoc
(
)
{
int
count
;
if
(
docWriter
!=
null
)
count
=
docWriter
.
getNumDocsInRAM
(
)
;
else
count
=
0
;
for
(
int
i
=
0
;
i
<
segmentInfos
.
size
(
)
;
i
++
)
count
+=
segmentInfos
.
info
(
i
)
.
docCount
;
return
count
;
}
public
synchronized
int
numDocs
(
)
throws
IOException
{
int
count
;
if
(
docWriter
!=
null
)
count
=
docWriter
.
getNumDocsInRAM
(
)
;
else
count
=
0
;
for
(
int
i
=
0
;
i
<
segmentInfos
.
size
(
)
;
i
++
)
{
final
SegmentInfo
info
=
segmentInfos
.
info
(
i
)
;
count
+=
info
.
docCount
-
info
.
getDelCount
(
)
;
}
return
count
;
}
public
synchronized
boolean
hasDeletions
(
)
throws
IOException
{
ensureOpen
(
)
;
if
(
docWriter
.
hasDeletes
(
)
)
return
true
;
for
(
int
i
=
0
;
i
<
segmentInfos
.
size
(
)
;
i
++
)
if
(
segmentInfos
.
info
(
i
)
.
hasDeletions
(
)
)
return
true
;
return
false
;
}
private
int
maxFieldLength
;
public
void
addDocument
(
Document
doc
)
throws
CorruptIndexException
,
IOException
{
addDocument
(
doc
,
analyzer
)
;
}
public
void
addDocument
(
Document
doc
,
Analyzer
analyzer
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
boolean
doFlush
=
false
;
boolean
success
=
false
;
try
{
try
{
doFlush
=
docWriter
.
addDocument
(
doc
,
analyzer
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception adding document"
)
;
synchronized
(
this
)
{
if
(
docWriter
!=
null
)
{
final
Collection
files
=
docWriter
.
abortedFiles
(
)
;
if
(
files
!=
null
)
deleter
.
deleteNewFiles
(
files
)
;
}
}
}
}
if
(
doFlush
)
flush
(
true
,
false
,
false
)
;
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
}
public
void
deleteDocuments
(
Term
term
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
try
{
boolean
doFlush
=
docWriter
.
bufferDeleteTerm
(
term
)
;
if
(
doFlush
)
flush
(
true
,
false
,
false
)
;
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
}
public
void
deleteDocuments
(
Term
[
]
terms
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
try
{
boolean
doFlush
=
docWriter
.
bufferDeleteTerms
(
terms
)
;
if
(
doFlush
)
flush
(
true
,
false
,
false
)
;
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
}
public
void
deleteDocuments
(
Query
query
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
boolean
doFlush
=
docWriter
.
bufferDeleteQuery
(
query
)
;
if
(
doFlush
)
flush
(
true
,
false
,
false
)
;
}
public
void
deleteDocuments
(
Query
[
]
queries
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
boolean
doFlush
=
docWriter
.
bufferDeleteQueries
(
queries
)
;
if
(
doFlush
)
flush
(
true
,
false
,
false
)
;
}
public
void
updateDocument
(
Term
term
,
Document
doc
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
updateDocument
(
term
,
doc
,
getAnalyzer
(
)
)
;
}
public
void
updateDocument
(
Term
term
,
Document
doc
,
Analyzer
analyzer
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
try
{
boolean
doFlush
=
false
;
boolean
success
=
false
;
try
{
doFlush
=
docWriter
.
updateDocument
(
term
,
doc
,
analyzer
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception updating document"
)
;
synchronized
(
this
)
{
final
Collection
files
=
docWriter
.
abortedFiles
(
)
;
if
(
files
!=
null
)
deleter
.
deleteNewFiles
(
files
)
;
}
}
}
if
(
doFlush
)
flush
(
true
,
false
,
false
)
;
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
}
final
synchronized
int
getSegmentCount
(
)
{
return
segmentInfos
.
size
(
)
;
}
final
synchronized
int
getNumBufferedDocuments
(
)
{
return
docWriter
.
getNumDocsInRAM
(
)
;
}
final
synchronized
int
getDocCount
(
int
i
)
{
if
(
i
>=
0
&&
i
<
segmentInfos
.
size
(
)
)
{
return
segmentInfos
.
info
(
i
)
.
docCount
;
}
else
{
return
-
1
;
}
}
final
synchronized
int
getFlushCount
(
)
{
return
flushCount
;
}
final
synchronized
int
getFlushDeletesCount
(
)
{
return
flushDeletesCount
;
}
final
String
newSegmentName
(
)
{
synchronized
(
segmentInfos
)
{
changeCount
++
;
return
"_"
+
Integer
.
toString
(
segmentInfos
.
counter
++
,
Character
.
MAX_RADIX
)
;
}
}
private
PrintStream
infoStream
=
null
;
private
static
PrintStream
defaultInfoStream
=
null
;
public
void
optimize
(
)
throws
CorruptIndexException
,
IOException
{
optimize
(
true
)
;
}
public
void
optimize
(
int
maxNumSegments
)
throws
CorruptIndexException
,
IOException
{
optimize
(
maxNumSegments
,
true
)
;
}
public
void
optimize
(
boolean
doWait
)
throws
CorruptIndexException
,
IOException
{
optimize
(
1
,
doWait
)
;
}
public
void
optimize
(
int
maxNumSegments
,
boolean
doWait
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
if
(
maxNumSegments
<
1
)
throw
new
IllegalArgumentException
(
"maxNumSegments must be >= 1; got "
+
maxNumSegments
)
;
if
(
infoStream
!=
null
)
message
(
"optimize: index now "
+
segString
(
)
)
;
flush
(
true
,
false
,
true
)
;
synchronized
(
this
)
{
resetMergeExceptions
(
)
;
segmentsToOptimize
=
new
HashSet
(
)
;
final
int
numSegments
=
segmentInfos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numSegments
;
i
++
)
segmentsToOptimize
.
add
(
segmentInfos
.
info
(
i
)
)
;
Iterator
it
=
pendingMerges
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
final
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
it
.
next
(
)
;
merge
.
optimize
=
true
;
merge
.
maxNumSegmentsOptimize
=
maxNumSegments
;
}
it
=
runningMerges
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
final
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
it
.
next
(
)
;
merge
.
optimize
=
true
;
merge
.
maxNumSegmentsOptimize
=
maxNumSegments
;
}
}
maybeMerge
(
maxNumSegments
,
true
)
;
if
(
doWait
)
{
synchronized
(
this
)
{
while
(
true
)
{
if
(
mergeExceptions
.
size
(
)
>
0
)
{
final
int
size
=
mergeExceptions
.
size
(
)
;
for
(
int
i
=
0
;
i
<
size
;
i
++
)
{
final
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
mergeExceptions
.
get
(
0
)
;
if
(
merge
.
optimize
)
{
IOException
err
=
new
IOException
(
"background merge hit exception: "
+
merge
.
segString
(
directory
)
)
;
final
Throwable
t
=
merge
.
getException
(
)
;
if
(
t
!=
null
)
err
.
initCause
(
t
)
;
throw
err
;
}
}
}
if
(
optimizeMergesPending
(
)
)
doWait
(
)
;
else
break
;
}
}
ensureOpen
(
)
;
}
}
private
synchronized
boolean
optimizeMergesPending
(
)
{
Iterator
it
=
pendingMerges
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
if
(
(
(
MergePolicy
.
OneMerge
)
it
.
next
(
)
)
.
optimize
)
return
true
;
it
=
runningMerges
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
if
(
(
(
MergePolicy
.
OneMerge
)
it
.
next
(
)
)
.
optimize
)
return
true
;
return
false
;
}
public
void
expungeDeletes
(
boolean
doWait
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
if
(
infoStream
!=
null
)
message
(
"expungeDeletes: index now "
+
segString
(
)
)
;
MergePolicy
.
MergeSpecification
spec
;
synchronized
(
this
)
{
spec
=
mergePolicy
.
findMergesToExpungeDeletes
(
segmentInfos
,
this
)
;
if
(
spec
!=
null
)
{
final
int
numMerges
=
spec
.
merges
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numMerges
;
i
++
)
registerMerge
(
(
MergePolicy
.
OneMerge
)
spec
.
merges
.
get
(
i
)
)
;
}
}
mergeScheduler
.
merge
(
this
)
;
if
(
spec
!=
null
&&
doWait
)
{
final
int
numMerges
=
spec
.
merges
.
size
(
)
;
synchronized
(
this
)
{
boolean
running
=
true
;
while
(
running
)
{
running
=
false
;
for
(
int
i
=
0
;
i
<
numMerges
;
i
++
)
{
final
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
spec
.
merges
.
get
(
i
)
;
if
(
pendingMerges
.
contains
(
merge
)
||
runningMerges
.
contains
(
merge
)
)
running
=
true
;
Throwable
t
=
merge
.
getException
(
)
;
if
(
t
!=
null
)
{
IOException
ioe
=
new
IOException
(
"background merge hit exception: "
+
merge
.
segString
(
directory
)
)
;
ioe
.
initCause
(
t
)
;
throw
ioe
;
}
}
if
(
running
)
doWait
(
)
;
}
}
}
}
public
void
expungeDeletes
(
)
throws
CorruptIndexException
,
IOException
{
expungeDeletes
(
true
)
;
}
public
final
void
maybeMerge
(
)
throws
CorruptIndexException
,
IOException
{
maybeMerge
(
false
)
;
}
private
final
void
maybeMerge
(
boolean
optimize
)
throws
CorruptIndexException
,
IOException
{
maybeMerge
(
1
,
optimize
)
;
}
private
final
void
maybeMerge
(
int
maxNumSegmentsOptimize
,
boolean
optimize
)
throws
CorruptIndexException
,
IOException
{
updatePendingMerges
(
maxNumSegmentsOptimize
,
optimize
)
;
mergeScheduler
.
merge
(
this
)
;
}
private
synchronized
void
updatePendingMerges
(
int
maxNumSegmentsOptimize
,
boolean
optimize
)
throws
CorruptIndexException
,
IOException
{
assert
!
optimize
||
maxNumSegmentsOptimize
>
0
;
if
(
stopMerges
)
return
;
final
MergePolicy
.
MergeSpecification
spec
;
if
(
optimize
)
{
spec
=
mergePolicy
.
findMergesForOptimize
(
segmentInfos
,
this
,
maxNumSegmentsOptimize
,
segmentsToOptimize
)
;
if
(
spec
!=
null
)
{
final
int
numMerges
=
spec
.
merges
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numMerges
;
i
++
)
{
final
MergePolicy
.
OneMerge
merge
=
(
(
MergePolicy
.
OneMerge
)
spec
.
merges
.
get
(
i
)
)
;
merge
.
optimize
=
true
;
merge
.
maxNumSegmentsOptimize
=
maxNumSegmentsOptimize
;
}
}
}
else
spec
=
mergePolicy
.
findMerges
(
segmentInfos
,
this
)
;
if
(
spec
!=
null
)
{
final
int
numMerges
=
spec
.
merges
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numMerges
;
i
++
)
registerMerge
(
(
MergePolicy
.
OneMerge
)
spec
.
merges
.
get
(
i
)
)
;
}
}
synchronized
MergePolicy
.
OneMerge
getNextMerge
(
)
{
if
(
pendingMerges
.
size
(
)
==
0
)
return
null
;
else
{
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
pendingMerges
.
removeFirst
(
)
;
runningMerges
.
add
(
merge
)
;
return
merge
;
}
}
private
synchronized
MergePolicy
.
OneMerge
getNextExternalMerge
(
)
{
if
(
pendingMerges
.
size
(
)
==
0
)
return
null
;
else
{
Iterator
it
=
pendingMerges
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
it
.
next
(
)
;
if
(
merge
.
isExternal
)
{
it
.
remove
(
)
;
runningMerges
.
add
(
merge
)
;
return
merge
;
}
}
return
null
;
}
}
private
synchronized
void
startTransaction
(
boolean
haveWriteLock
)
throws
IOException
{
boolean
success
=
false
;
try
{
if
(
infoStream
!=
null
)
message
(
"now start transaction"
)
;
assert
docWriter
.
getNumBufferedDeleteTerms
(
)
==
0
:
"calling startTransaction with buffered delete terms not supported: numBufferedDeleteTerms="
+
docWriter
.
getNumBufferedDeleteTerms
(
)
;
assert
docWriter
.
getNumDocsInRAM
(
)
==
0
:
"calling startTransaction with buffered documents not supported: numDocsInRAM="
+
docWriter
.
getNumDocsInRAM
(
)
;
ensureOpen
(
)
;
synchronized
(
this
)
{
while
(
stopMerges
)
doWait
(
)
;
}
success
=
true
;
}
finally
{
if
(
!
success
&&
haveWriteLock
)
releaseWrite
(
)
;
}
if
(
!
haveWriteLock
)
acquireWrite
(
)
;
success
=
false
;
try
{
localRollbackSegmentInfos
=
(
SegmentInfos
)
segmentInfos
.
clone
(
)
;
assert
!
hasExternalSegments
(
segmentInfos
)
;
localAutoCommit
=
autoCommit
;
localFlushedDocCount
=
docWriter
.
getFlushedDocCount
(
)
;
if
(
localAutoCommit
)
{
if
(
infoStream
!=
null
)
message
(
"flush at startTransaction"
)
;
flush
(
true
,
false
,
false
)
;
autoCommit
=
false
;
}
else
deleter
.
incRef
(
segmentInfos
,
false
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
finishAddIndexes
(
)
;
}
}
private
synchronized
void
rollbackTransaction
(
)
throws
IOException
{
if
(
infoStream
!=
null
)
message
(
"now rollback transaction"
)
;
autoCommit
=
localAutoCommit
;
docWriter
.
setFlushedDocCount
(
localFlushedDocCount
)
;
finishMerges
(
false
)
;
segmentInfos
.
clear
(
)
;
segmentInfos
.
addAll
(
localRollbackSegmentInfos
)
;
localRollbackSegmentInfos
=
null
;
finishAddIndexes
(
)
;
deleter
.
checkpoint
(
segmentInfos
,
false
)
;
if
(
!
autoCommit
)
deleter
.
decRef
(
segmentInfos
)
;
deleter
.
refresh
(
)
;
notifyAll
(
)
;
assert
!
hasExternalSegments
(
)
;
}
private
synchronized
void
commitTransaction
(
)
throws
IOException
{
if
(
infoStream
!=
null
)
message
(
"now commit transaction"
)
;
autoCommit
=
localAutoCommit
;
checkpoint
(
)
;
if
(
autoCommit
)
{
boolean
success
=
false
;
try
{
commit
(
0
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception committing transaction"
)
;
rollbackTransaction
(
)
;
}
}
}
else
deleter
.
decRef
(
localRollbackSegmentInfos
)
;
localRollbackSegmentInfos
=
null
;
assert
!
hasExternalSegments
(
)
;
finishAddIndexes
(
)
;
}
public
void
abort
(
)
throws
IOException
{
rollback
(
)
;
}
public
void
rollback
(
)
throws
IOException
{
ensureOpen
(
)
;
if
(
autoCommit
)
throw
new
IllegalStateException
(
"rollback() can only be called when IndexWriter was opened with autoCommit=false"
)
;
if
(
shouldClose
(
)
)
rollbackInternal
(
)
;
}
private
void
rollbackInternal
(
)
throws
IOException
{
boolean
success
=
false
;
docWriter
.
pauseAllThreads
(
)
;
try
{
finishMerges
(
false
)
;
mergePolicy
.
close
(
)
;
mergeScheduler
.
close
(
)
;
synchronized
(
this
)
{
if
(
pendingCommit
!=
null
)
{
pendingCommit
.
rollbackCommit
(
directory
)
;
deleter
.
decRef
(
pendingCommit
)
;
pendingCommit
=
null
;
notifyAll
(
)
;
}
segmentInfos
.
clear
(
)
;
segmentInfos
.
addAll
(
rollbackSegmentInfos
)
;
assert
!
hasExternalSegments
(
)
;
docWriter
.
abort
(
)
;
assert
testPoint
(
"rollback before checkpoint"
)
;
deleter
.
checkpoint
(
segmentInfos
,
false
)
;
deleter
.
refresh
(
)
;
}
lastCommitChangeCount
=
changeCount
;
success
=
true
;
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
finally
{
synchronized
(
this
)
{
if
(
!
success
)
{
docWriter
.
resumeAllThreads
(
)
;
closing
=
false
;
notifyAll
(
)
;
if
(
infoStream
!=
null
)
message
(
"hit exception during rollback"
)
;
}
}
}
closeInternal
(
false
)
;
}
private
synchronized
void
finishMerges
(
boolean
waitForMerges
)
throws
IOException
{
if
(
!
waitForMerges
)
{
stopMerges
=
true
;
Iterator
it
=
pendingMerges
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
final
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
it
.
next
(
)
;
if
(
infoStream
!=
null
)
message
(
"now abort pending merge "
+
merge
.
segString
(
directory
)
)
;
merge
.
abort
(
)
;
mergeFinish
(
merge
)
;
}
pendingMerges
.
clear
(
)
;
it
=
runningMerges
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
final
MergePolicy
.
OneMerge
merge
=
(
MergePolicy
.
OneMerge
)
it
.
next
(
)
;
if
(
infoStream
!=
null
)
message
(
"now abort running merge "
+
merge
.
segString
(
directory
)
)
;
merge
.
abort
(
)
;
}
acquireRead
(
)
;
releaseRead
(
)
;
while
(
runningMerges
.
size
(
)
>
0
)
{
if
(
infoStream
!=
null
)
message
(
"now wait for "
+
runningMerges
.
size
(
)
+
" running merge to abort"
)
;
doWait
(
)
;
}
stopMerges
=
false
;
notifyAll
(
)
;
assert
0
==
mergingSegments
.
size
(
)
;
if
(
infoStream
!=
null
)
message
(
"all running merges have aborted"
)
;
}
else
{
acquireRead
(
)
;
releaseRead
(
)
;
while
(
pendingMerges
.
size
(
)
>
0
||
runningMerges
.
size
(
)
>
0
)
doWait
(
)
;
assert
0
==
mergingSegments
.
size
(
)
;
}
}
private
synchronized
void
checkpoint
(
)
throws
IOException
{
changeCount
++
;
deleter
.
checkpoint
(
segmentInfos
,
false
)
;
}
private
void
finishAddIndexes
(
)
{
releaseWrite
(
)
;
}
private
void
blockAddIndexes
(
boolean
includePendingClose
)
{
acquireRead
(
)
;
boolean
success
=
false
;
try
{
ensureOpen
(
includePendingClose
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
releaseRead
(
)
;
}
}
private
void
resumeAddIndexes
(
)
{
releaseRead
(
)
;
}
public
void
addIndexes
(
Directory
[
]
dirs
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
noDupDirs
(
dirs
)
;
docWriter
.
pauseAllThreads
(
)
;
try
{
if
(
infoStream
!=
null
)
message
(
"flush at addIndexes"
)
;
flush
(
true
,
false
,
true
)
;
boolean
success
=
false
;
startTransaction
(
false
)
;
try
{
int
docCount
=
0
;
synchronized
(
this
)
{
ensureOpen
(
)
;
for
(
int
i
=
0
;
i
<
dirs
.
length
;
i
++
)
{
SegmentInfos
sis
=
new
SegmentInfos
(
)
;
sis
.
read
(
dirs
[
i
]
)
;
for
(
int
j
=
0
;
j
<
sis
.
size
(
)
;
j
++
)
{
final
SegmentInfo
info
=
sis
.
info
(
j
)
;
docCount
+=
info
.
docCount
;
assert
!
segmentInfos
.
contains
(
info
)
;
segmentInfos
.
add
(
info
)
;
}
}
}
docWriter
.
updateFlushedDocCount
(
docCount
)
;
optimize
(
)
;
success
=
true
;
}
finally
{
if
(
success
)
{
commitTransaction
(
)
;
}
else
{
rollbackTransaction
(
)
;
}
}
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
finally
{
docWriter
.
resumeAllThreads
(
)
;
}
}
private
synchronized
void
resetMergeExceptions
(
)
{
mergeExceptions
=
new
ArrayList
(
)
;
mergeGen
++
;
}
private
void
noDupDirs
(
Directory
[
]
dirs
)
{
HashSet
dups
=
new
HashSet
(
)
;
for
(
int
i
=
0
;
i
<
dirs
.
length
;
i
++
)
{
if
(
dups
.
contains
(
dirs
[
i
]
)
)
throw
new
IllegalArgumentException
(
"Directory "
+
dirs
[
i
]
+
" appears more than once"
)
;
if
(
dirs
[
i
]
==
directory
)
throw
new
IllegalArgumentException
(
"Cannot add directory to itself"
)
;
dups
.
add
(
dirs
[
i
]
)
;
}
}
public
void
addIndexesNoOptimize
(
Directory
[
]
dirs
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
noDupDirs
(
dirs
)
;
docWriter
.
pauseAllThreads
(
)
;
try
{
if
(
infoStream
!=
null
)
message
(
"flush at addIndexesNoOptimize"
)
;
flush
(
true
,
false
,
true
)
;
boolean
success
=
false
;
startTransaction
(
false
)
;
try
{
int
docCount
=
0
;
synchronized
(
this
)
{
ensureOpen
(
)
;
for
(
int
i
=
0
;
i
<
dirs
.
length
;
i
++
)
{
if
(
directory
==
dirs
[
i
]
)
{
throw
new
IllegalArgumentException
(
"Cannot add this index to itself"
)
;
}
SegmentInfos
sis
=
new
SegmentInfos
(
)
;
sis
.
read
(
dirs
[
i
]
)
;
for
(
int
j
=
0
;
j
<
sis
.
size
(
)
;
j
++
)
{
SegmentInfo
info
=
sis
.
info
(
j
)
;
assert
!
segmentInfos
.
contains
(
info
)
:
"dup info dir="
+
info
.
dir
+
" name="
+
info
.
name
;
docCount
+=
info
.
docCount
;
segmentInfos
.
add
(
info
)
;
}
}
}
docWriter
.
updateFlushedDocCount
(
docCount
)
;
maybeMerge
(
)
;
ensureOpen
(
)
;
resolveExternalSegments
(
)
;
ensureOpen
(
)
;
success
=
true
;
}
finally
{
if
(
success
)
{
commitTransaction
(
)
;
}
else
{
rollbackTransaction
(
)
;
}
}
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
finally
{
docWriter
.
resumeAllThreads
(
)
;
}
}
private
boolean
hasExternalSegments
(
)
{
return
hasExternalSegments
(
segmentInfos
)
;
}
private
boolean
hasExternalSegments
(
SegmentInfos
infos
)
{
final
int
numSegments
=
infos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numSegments
;
i
++
)
if
(
infos
.
info
(
i
)
.
dir
!=
directory
)
return
true
;
return
false
;
}
private
void
resolveExternalSegments
(
)
throws
CorruptIndexException
,
IOException
{
boolean
any
=
false
;
boolean
done
=
false
;
while
(
!
done
)
{
SegmentInfo
info
=
null
;
MergePolicy
.
OneMerge
merge
=
null
;
synchronized
(
this
)
{
if
(
stopMerges
)
throw
new
MergePolicy
.
MergeAbortedException
(
"rollback() was called or addIndexes* hit an unhandled exception"
)
;
final
int
numSegments
=
segmentInfos
.
size
(
)
;
done
=
true
;
for
(
int
i
=
0
;
i
<
numSegments
;
i
++
)
{
info
=
segmentInfos
.
info
(
i
)
;
if
(
info
.
dir
!=
directory
)
{
done
=
false
;
final
MergePolicy
.
OneMerge
newMerge
=
new
MergePolicy
.
OneMerge
(
segmentInfos
.
range
(
i
,
1
+
i
)
,
info
.
getUseCompoundFile
(
)
)
;
if
(
registerMerge
(
newMerge
)
)
{
merge
=
newMerge
;
pendingMerges
.
remove
(
merge
)
;
runningMerges
.
add
(
merge
)
;
break
;
}
}
}
if
(
!
done
&&
merge
==
null
)
merge
=
getNextExternalMerge
(
)
;
if
(
!
done
&&
merge
==
null
)
doWait
(
)
;
}
if
(
merge
!=
null
)
{
any
=
true
;
merge
(
merge
)
;
}
}
if
(
any
)
mergeScheduler
.
merge
(
this
)
;
}
public
void
addIndexes
(
IndexReader
[
]
readers
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
docWriter
.
pauseAllThreads
(
)
;
acquireWrite
(
)
;
try
{
boolean
success
=
false
;
SegmentInfo
info
=
null
;
String
mergedName
=
null
;
SegmentMerger
merger
=
null
;
try
{
flush
(
true
,
false
,
true
)
;
optimize
(
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
releaseWrite
(
)
;
}
startTransaction
(
true
)
;
try
{
mergedName
=
newSegmentName
(
)
;
merger
=
new
SegmentMerger
(
this
,
mergedName
,
null
)
;
IndexReader
sReader
=
null
;
synchronized
(
this
)
{
if
(
segmentInfos
.
size
(
)
==
1
)
{
sReader
=
SegmentReader
.
get
(
true
,
segmentInfos
.
info
(
0
)
)
;
}
}
try
{
if
(
sReader
!=
null
)
merger
.
add
(
sReader
)
;
for
(
int
i
=
0
;
i
<
readers
.
length
;
i
++
)
merger
.
add
(
readers
[
i
]
)
;
int
docCount
=
merger
.
merge
(
)
;
if
(
sReader
!=
null
)
{
sReader
.
close
(
)
;
sReader
=
null
;
}
synchronized
(
this
)
{
segmentInfos
.
clear
(
)
;
info
=
new
SegmentInfo
(
mergedName
,
docCount
,
directory
,
false
,
true
,
-
1
,
null
,
false
,
merger
.
hasProx
(
)
)
;
segmentInfos
.
add
(
info
)
;
}
docWriter
.
updateFlushedDocCount
(
docCount
)
;
success
=
true
;
}
finally
{
if
(
sReader
!=
null
)
{
sReader
.
close
(
)
;
}
}
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception in addIndexes during merge"
)
;
rollbackTransaction
(
)
;
}
else
{
commitTransaction
(
)
;
}
}
if
(
mergePolicy
instanceof
LogMergePolicy
&&
getUseCompoundFile
(
)
)
{
List
files
=
null
;
synchronized
(
this
)
{
if
(
segmentInfos
.
contains
(
info
)
)
{
files
=
info
.
files
(
)
;
deleter
.
incRef
(
files
)
;
}
}
if
(
files
!=
null
)
{
success
=
false
;
startTransaction
(
false
)
;
try
{
merger
.
createCompoundFile
(
mergedName
+
".cfs"
)
;
synchronized
(
this
)
{
info
.
setUseCompoundFile
(
true
)
;
}
success
=
true
;
}
finally
{
deleter
.
decRef
(
files
)
;
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception building compound file in addIndexes during merge"
)
;
rollbackTransaction
(
)
;
}
else
{
commitTransaction
(
)
;
}
}
}
}
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
finally
{
docWriter
.
resumeAllThreads
(
)
;
}
}
void
doAfterFlush
(
)
throws
IOException
{
}
public
final
void
flush
(
)
throws
CorruptIndexException
,
IOException
{
flush
(
true
,
false
,
true
)
;
}
public
final
void
prepareCommit
(
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
prepareCommit
(
false
)
;
}
private
final
void
prepareCommit
(
boolean
internal
)
throws
CorruptIndexException
,
IOException
{
if
(
hitOOM
)
throw
new
IllegalStateException
(
"this writer hit an OutOfMemoryError; cannot commit"
)
;
if
(
autoCommit
&&
!
internal
)
throw
new
IllegalStateException
(
"this method can only be used when autoCommit is false"
)
;
if
(
!
autoCommit
&&
pendingCommit
!=
null
)
throw
new
IllegalStateException
(
"prepareCommit was already called with no corresponding call to commit"
)
;
message
(
"prepareCommit: flush"
)
;
flush
(
true
,
true
,
true
)
;
startCommit
(
0
)
;
}
private
void
commit
(
long
sizeInBytes
)
throws
IOException
{
startCommit
(
sizeInBytes
)
;
finishCommit
(
)
;
}
private
boolean
committing
;
synchronized
private
void
waitForCommit
(
)
{
while
(
committing
)
doWait
(
)
;
committing
=
true
;
}
synchronized
private
void
doneCommit
(
)
{
committing
=
false
;
notifyAll
(
)
;
}
public
final
void
commit
(
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
waitForCommit
(
)
;
try
{
message
(
"commit: start"
)
;
if
(
autoCommit
||
pendingCommit
==
null
)
{
message
(
"commit: now prepare"
)
;
prepareCommit
(
true
)
;
}
else
message
(
"commit: already prepared"
)
;
finishCommit
(
)
;
}
finally
{
doneCommit
(
)
;
}
}
private
synchronized
final
void
finishCommit
(
)
throws
CorruptIndexException
,
IOException
{
if
(
pendingCommit
!=
null
)
{
try
{
message
(
"commit: pendingCommit != null"
)
;
pendingCommit
.
finishCommit
(
directory
)
;
lastCommitChangeCount
=
pendingCommitChangeCount
;
segmentInfos
.
updateGeneration
(
pendingCommit
)
;
setRollbackSegmentInfos
(
pendingCommit
)
;
deleter
.
checkpoint
(
pendingCommit
,
true
)
;
}
finally
{
deleter
.
decRef
(
pendingCommit
)
;
pendingCommit
=
null
;
notifyAll
(
)
;
}
}
else
message
(
"commit: pendingCommit == null; skip"
)
;
message
(
"commit: done"
)
;
}
protected
final
void
flush
(
boolean
triggerMerge
,
boolean
flushDocStores
,
boolean
flushDeletes
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
false
)
;
if
(
doFlush
(
flushDocStores
,
flushDeletes
)
&&
triggerMerge
)
maybeMerge
(
)
;
}
private
synchronized
final
boolean
doFlush
(
boolean
flushDocStores
,
boolean
flushDeletes
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
false
)
;
assert
testPoint
(
"startDoFlush"
)
;
flushCount
++
;
flushDeletes
|=
docWriter
.
deletesFull
(
)
;
flushDeletes
|=
autoCommit
;
if
(
docWriter
.
pauseAllThreads
(
)
)
{
docWriter
.
resumeAllThreads
(
)
;
return
false
;
}
try
{
SegmentInfo
newSegment
=
null
;
final
int
numDocs
=
docWriter
.
getNumDocsInRAM
(
)
;
boolean
flushDocs
=
numDocs
>
0
;
flushDocStores
|=
autoCommit
;
String
docStoreSegment
=
docWriter
.
getDocStoreSegment
(
)
;
if
(
docStoreSegment
==
null
)
flushDocStores
=
false
;
int
docStoreOffset
=
docWriter
.
getDocStoreOffset
(
)
;
assert
!
autoCommit
||
0
==
docStoreOffset
;
boolean
docStoreIsCompoundFile
=
false
;
if
(
infoStream
!=
null
)
{
message
(
"  flush: segment="
+
docWriter
.
getSegment
(
)
+
" docStoreSegment="
+
docWriter
.
getDocStoreSegment
(
)
+
" docStoreOffset="
+
docStoreOffset
+
" flushDocs="
+
flushDocs
+
" flushDeletes="
+
flushDeletes
+
" flushDocStores="
+
flushDocStores
+
" numDocs="
+
numDocs
+
" numBufDelTerms="
+
docWriter
.
getNumBufferedDeleteTerms
(
)
)
;
message
(
"  index before flush "
+
segString
(
)
)
;
}
if
(
flushDocStores
&&
(
!
flushDocs
||
!
docWriter
.
getSegment
(
)
.
equals
(
docWriter
.
getDocStoreSegment
(
)
)
)
)
{
if
(
infoStream
!=
null
)
message
(
"  flush shared docStore segment "
+
docStoreSegment
)
;
docStoreIsCompoundFile
=
flushDocStores
(
)
;
flushDocStores
=
false
;
}
String
segment
=
docWriter
.
getSegment
(
)
;
assert
segment
!=
null
||
!
flushDocs
;
if
(
flushDocs
)
{
boolean
success
=
false
;
final
int
flushedDocCount
;
try
{
flushedDocCount
=
docWriter
.
flush
(
flushDocStores
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception flushing segment "
+
segment
)
;
deleter
.
refresh
(
segment
)
;
}
}
if
(
0
==
docStoreOffset
&&
flushDocStores
)
{
assert
docStoreSegment
!=
null
;
assert
docStoreSegment
.
equals
(
segment
)
;
docStoreOffset
=
-
1
;
docStoreIsCompoundFile
=
false
;
docStoreSegment
=
null
;
}
newSegment
=
new
SegmentInfo
(
segment
,
flushedDocCount
,
directory
,
false
,
true
,
docStoreOffset
,
docStoreSegment
,
docStoreIsCompoundFile
,
docWriter
.
hasProx
(
)
)
;
}
docWriter
.
pushDeletes
(
)
;
if
(
flushDocs
)
segmentInfos
.
add
(
newSegment
)
;
if
(
flushDeletes
)
{
flushDeletesCount
++
;
applyDeletes
(
)
;
}
doAfterFlush
(
)
;
if
(
flushDocs
)
checkpoint
(
)
;
if
(
flushDocs
&&
mergePolicy
.
useCompoundFile
(
segmentInfos
,
newSegment
)
)
{
boolean
success
=
false
;
try
{
docWriter
.
createCompoundFile
(
segment
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception creating compound file for newly flushed segment "
+
segment
)
;
deleter
.
deleteFile
(
segment
+
"."
+
IndexFileNames
.
COMPOUND_FILE_EXTENSION
)
;
}
}
newSegment
.
setUseCompoundFile
(
true
)
;
checkpoint
(
)
;
}
return
flushDocs
;
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
finally
{
docWriter
.
clearFlushPending
(
)
;
docWriter
.
resumeAllThreads
(
)
;
}
}
public
final
long
ramSizeInBytes
(
)
{
ensureOpen
(
)
;
return
docWriter
.
getRAMUsed
(
)
;
}
public
final
synchronized
int
numRamDocs
(
)
{
ensureOpen
(
)
;
return
docWriter
.
getNumDocsInRAM
(
)
;
}
private
int
ensureContiguousMerge
(
MergePolicy
.
OneMerge
merge
)
{
int
first
=
segmentInfos
.
indexOf
(
merge
.
segments
.
info
(
0
)
)
;
if
(
first
==
-
1
)
throw
new
MergePolicy
.
MergeException
(
"could not find segment "
+
merge
.
segments
.
info
(
0
)
.
name
+
" in current segments"
,
directory
)
;
final
int
numSegments
=
segmentInfos
.
size
(
)
;
final
int
numSegmentsToMerge
=
merge
.
segments
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numSegmentsToMerge
;
i
++
)
{
final
SegmentInfo
info
=
merge
.
segments
.
info
(
i
)
;
if
(
first
+
i
>=
numSegments
||
!
segmentInfos
.
info
(
first
+
i
)
.
equals
(
info
)
)
{
if
(
segmentInfos
.
indexOf
(
info
)
==
-
1
)
throw
new
MergePolicy
.
MergeException
(
"MergePolicy selected a segment ("
+
info
.
name
+
") that is not in the index"
,
directory
)
;
else
throw
new
MergePolicy
.
MergeException
(
"MergePolicy selected non-contiguous segments to merge ("
+
merge
.
segString
(
directory
)
+
" vs "
+
segString
(
)
+
"), which IndexWriter (currently) cannot handle"
,
directory
)
;
}
}
return
first
;
}
synchronized
private
void
commitMergedDeletes
(
MergePolicy
.
OneMerge
merge
)
throws
IOException
{
assert
testPoint
(
"startCommitMergeDeletes"
)
;
final
SegmentInfos
sourceSegmentsClone
=
merge
.
segmentsClone
;
final
SegmentInfos
sourceSegments
=
merge
.
segments
;
if
(
infoStream
!=
null
)
message
(
"commitMergeDeletes "
+
merge
.
segString
(
directory
)
)
;
BitVector
deletes
=
null
;
int
docUpto
=
0
;
int
delCount
=
0
;
final
int
numSegmentsToMerge
=
sourceSegments
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numSegmentsToMerge
;
i
++
)
{
final
SegmentInfo
previousInfo
=
sourceSegmentsClone
.
info
(
i
)
;
final
SegmentInfo
currentInfo
=
sourceSegments
.
info
(
i
)
;
assert
currentInfo
.
docCount
==
previousInfo
.
docCount
;
final
int
docCount
=
currentInfo
.
docCount
;
if
(
previousInfo
.
hasDeletions
(
)
)
{
assert
currentInfo
.
hasDeletions
(
)
;
BitVector
previousDeletes
=
new
BitVector
(
previousInfo
.
dir
,
previousInfo
.
getDelFileName
(
)
)
;
if
(
!
currentInfo
.
getDelFileName
(
)
.
equals
(
previousInfo
.
getDelFileName
(
)
)
)
{
if
(
deletes
==
null
)
deletes
=
new
BitVector
(
merge
.
info
.
docCount
)
;
BitVector
currentDeletes
=
new
BitVector
(
currentInfo
.
dir
,
currentInfo
.
getDelFileName
(
)
)
;
for
(
int
j
=
0
;
j
<
docCount
;
j
++
)
{
if
(
previousDeletes
.
get
(
j
)
)
assert
currentDeletes
.
get
(
j
)
;
else
{
if
(
currentDeletes
.
get
(
j
)
)
{
deletes
.
set
(
docUpto
)
;
delCount
++
;
}
docUpto
++
;
}
}
}
else
docUpto
+=
docCount
-
previousDeletes
.
count
(
)
;
}
else
if
(
currentInfo
.
hasDeletions
(
)
)
{
if
(
deletes
==
null
)
deletes
=
new
BitVector
(
merge
.
info
.
docCount
)
;
BitVector
currentDeletes
=
new
BitVector
(
directory
,
currentInfo
.
getDelFileName
(
)
)
;
for
(
int
j
=
0
;
j
<
docCount
;
j
++
)
{
if
(
currentDeletes
.
get
(
j
)
)
{
deletes
.
set
(
docUpto
)
;
delCount
++
;
}
docUpto
++
;
}
}
else
docUpto
+=
currentInfo
.
docCount
;
}
if
(
deletes
!=
null
)
{
merge
.
info
.
advanceDelGen
(
)
;
message
(
"commit merge deletes to "
+
merge
.
info
.
getDelFileName
(
)
)
;
deletes
.
write
(
directory
,
merge
.
info
.
getDelFileName
(
)
)
;
merge
.
info
.
setDelCount
(
delCount
)
;
assert
delCount
==
deletes
.
count
(
)
;
}
}
synchronized
private
boolean
commitMerge
(
MergePolicy
.
OneMerge
merge
,
SegmentMerger
merger
,
int
mergedDocCount
)
throws
IOException
{
assert
testPoint
(
"startCommitMerge"
)
;
if
(
hitOOM
)
return
false
;
if
(
infoStream
!=
null
)
message
(
"commitMerge: "
+
merge
.
segString
(
directory
)
+
" index="
+
segString
(
)
)
;
assert
merge
.
registerDone
;
if
(
merge
.
isAborted
(
)
)
{
if
(
infoStream
!=
null
)
message
(
"commitMerge: skipping merge "
+
merge
.
segString
(
directory
)
+
": it was aborted"
)
;
deleter
.
refresh
(
merge
.
info
.
name
)
;
return
false
;
}
final
int
start
=
ensureContiguousMerge
(
merge
)
;
commitMergedDeletes
(
merge
)
;
docWriter
.
remapDeletes
(
segmentInfos
,
merger
.
getDocMaps
(
)
,
merger
.
getDelCounts
(
)
,
merge
,
mergedDocCount
)
;
final
String
mergeDocStoreSegment
=
merge
.
info
.
getDocStoreSegment
(
)
;
if
(
mergeDocStoreSegment
!=
null
&&
!
merge
.
info
.
getDocStoreIsCompoundFile
(
)
)
{
final
int
size
=
segmentInfos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
size
;
i
++
)
{
final
SegmentInfo
info
=
segmentInfos
.
info
(
i
)
;
final
String
docStoreSegment
=
info
.
getDocStoreSegment
(
)
;
if
(
docStoreSegment
!=
null
&&
docStoreSegment
.
equals
(
mergeDocStoreSegment
)
&&
info
.
getDocStoreIsCompoundFile
(
)
)
{
merge
.
info
.
setDocStoreIsCompoundFile
(
true
)
;
break
;
}
}
}
merge
.
info
.
setHasProx
(
merger
.
hasProx
(
)
)
;
segmentInfos
.
subList
(
start
,
start
+
merge
.
segments
.
size
(
)
)
.
clear
(
)
;
assert
!
segmentInfos
.
contains
(
merge
.
info
)
;
segmentInfos
.
add
(
start
,
merge
.
info
)
;
checkpoint
(
)
;
decrefMergeSegments
(
merge
)
;
if
(
merge
.
optimize
)
segmentsToOptimize
.
add
(
merge
.
info
)
;
return
true
;
}
private
void
decrefMergeSegments
(
MergePolicy
.
OneMerge
merge
)
throws
IOException
{
final
SegmentInfos
sourceSegmentsClone
=
merge
.
segmentsClone
;
final
int
numSegmentsToMerge
=
sourceSegmentsClone
.
size
(
)
;
assert
merge
.
increfDone
;
merge
.
increfDone
=
false
;
for
(
int
i
=
0
;
i
<
numSegmentsToMerge
;
i
++
)
{
final
SegmentInfo
previousInfo
=
sourceSegmentsClone
.
info
(
i
)
;
if
(
previousInfo
.
dir
==
directory
)
deleter
.
decRef
(
previousInfo
.
files
(
)
)
;
}
}
final
private
void
handleMergeException
(
Throwable
t
,
MergePolicy
.
OneMerge
merge
)
throws
IOException
{
merge
.
setException
(
t
)
;
addMergeException
(
merge
)
;
if
(
t
instanceof
MergePolicy
.
MergeAbortedException
)
{
if
(
merge
.
isExternal
)
throw
(
MergePolicy
.
MergeAbortedException
)
t
;
}
else
if
(
t
instanceof
IOException
)
throw
(
IOException
)
t
;
else
if
(
t
instanceof
RuntimeException
)
throw
(
RuntimeException
)
t
;
else
if
(
t
instanceof
Error
)
throw
(
Error
)
t
;
else
throw
new
RuntimeException
(
t
)
;
}
final
void
merge
(
MergePolicy
.
OneMerge
merge
)
throws
CorruptIndexException
,
IOException
{
boolean
success
=
false
;
try
{
try
{
try
{
mergeInit
(
merge
)
;
if
(
infoStream
!=
null
)
message
(
"now merge\n  merge="
+
merge
.
segString
(
directory
)
+
"\n  merge="
+
merge
+
"\n  index="
+
segString
(
)
)
;
mergeMiddle
(
merge
)
;
success
=
true
;
}
catch
(
Throwable
t
)
{
handleMergeException
(
t
,
merge
)
;
}
}
finally
{
synchronized
(
this
)
{
try
{
mergeFinish
(
merge
)
;
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception during merge"
)
;
if
(
merge
.
info
!=
null
&&
!
segmentInfos
.
contains
(
merge
.
info
)
)
deleter
.
refresh
(
merge
.
info
.
name
)
;
}
if
(
success
&&
!
merge
.
isAborted
(
)
&&
!
closed
&&
!
closing
)
updatePendingMerges
(
merge
.
maxNumSegmentsOptimize
,
merge
.
optimize
)
;
}
finally
{
runningMerges
.
remove
(
merge
)
;
}
}
}
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
}
final
synchronized
boolean
registerMerge
(
MergePolicy
.
OneMerge
merge
)
throws
MergePolicy
.
MergeAbortedException
{
if
(
merge
.
registerDone
)
return
true
;
if
(
stopMerges
)
{
merge
.
abort
(
)
;
throw
new
MergePolicy
.
MergeAbortedException
(
"merge is aborted: "
+
merge
.
segString
(
directory
)
)
;
}
final
int
count
=
merge
.
segments
.
size
(
)
;
boolean
isExternal
=
false
;
for
(
int
i
=
0
;
i
<
count
;
i
++
)
{
final
SegmentInfo
info
=
merge
.
segments
.
info
(
i
)
;
if
(
mergingSegments
.
contains
(
info
)
)
return
false
;
if
(
segmentInfos
.
indexOf
(
info
)
==
-
1
)
return
false
;
if
(
info
.
dir
!=
directory
)
isExternal
=
true
;
}
ensureContiguousMerge
(
merge
)
;
pendingMerges
.
add
(
merge
)
;
if
(
infoStream
!=
null
)
message
(
"add merge to pendingMerges: "
+
merge
.
segString
(
directory
)
+
" [total "
+
pendingMerges
.
size
(
)
+
" pending]"
)
;
merge
.
mergeGen
=
mergeGen
;
merge
.
isExternal
=
isExternal
;
for
(
int
i
=
0
;
i
<
count
;
i
++
)
mergingSegments
.
add
(
merge
.
segments
.
info
(
i
)
)
;
merge
.
registerDone
=
true
;
return
true
;
}
final
synchronized
void
mergeInit
(
MergePolicy
.
OneMerge
merge
)
throws
IOException
{
boolean
success
=
false
;
try
{
_mergeInit
(
merge
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
mergeFinish
(
merge
)
;
runningMerges
.
remove
(
merge
)
;
}
}
}
final
synchronized
private
void
_mergeInit
(
MergePolicy
.
OneMerge
merge
)
throws
IOException
{
assert
testPoint
(
"startMergeInit"
)
;
assert
merge
.
registerDone
;
assert
!
merge
.
optimize
||
merge
.
maxNumSegmentsOptimize
>
0
;
if
(
merge
.
info
!=
null
)
return
;
if
(
merge
.
isAborted
(
)
)
return
;
boolean
changed
=
applyDeletes
(
)
;
assert
!
changed
||
!
autoCommit
;
final
SegmentInfos
sourceSegments
=
merge
.
segments
;
final
int
end
=
sourceSegments
.
size
(
)
;
Directory
lastDir
=
directory
;
String
lastDocStoreSegment
=
null
;
int
next
=
-
1
;
boolean
mergeDocStores
=
false
;
boolean
doFlushDocStore
=
false
;
final
String
currentDocStoreSegment
=
docWriter
.
getDocStoreSegment
(
)
;
for
(
int
i
=
0
;
i
<
end
;
i
++
)
{
SegmentInfo
si
=
sourceSegments
.
info
(
i
)
;
if
(
si
.
hasDeletions
(
)
)
mergeDocStores
=
true
;
if
(
-
1
==
si
.
getDocStoreOffset
(
)
)
mergeDocStores
=
true
;
String
docStoreSegment
=
si
.
getDocStoreSegment
(
)
;
if
(
docStoreSegment
==
null
)
mergeDocStores
=
true
;
else
if
(
lastDocStoreSegment
==
null
)
lastDocStoreSegment
=
docStoreSegment
;
else
if
(
!
lastDocStoreSegment
.
equals
(
docStoreSegment
)
)
mergeDocStores
=
true
;
if
(
-
1
==
next
)
next
=
si
.
getDocStoreOffset
(
)
+
si
.
docCount
;
else
if
(
next
!=
si
.
getDocStoreOffset
(
)
)
mergeDocStores
=
true
;
else
next
=
si
.
getDocStoreOffset
(
)
+
si
.
docCount
;
if
(
lastDir
!=
si
.
dir
)
mergeDocStores
=
true
;
if
(
si
.
getDocStoreOffset
(
)
!=
-
1
&&
currentDocStoreSegment
!=
null
&&
si
.
getDocStoreSegment
(
)
.
equals
(
currentDocStoreSegment
)
)
{
doFlushDocStore
=
true
;
}
}
final
int
docStoreOffset
;
final
String
docStoreSegment
;
final
boolean
docStoreIsCompoundFile
;
if
(
mergeDocStores
)
{
docStoreOffset
=
-
1
;
docStoreSegment
=
null
;
docStoreIsCompoundFile
=
false
;
}
else
{
SegmentInfo
si
=
sourceSegments
.
info
(
0
)
;
docStoreOffset
=
si
.
getDocStoreOffset
(
)
;
docStoreSegment
=
si
.
getDocStoreSegment
(
)
;
docStoreIsCompoundFile
=
si
.
getDocStoreIsCompoundFile
(
)
;
}
if
(
mergeDocStores
&&
doFlushDocStore
)
{
if
(
infoStream
!=
null
)
message
(
"now flush at merge"
)
;
doFlush
(
true
,
false
)
;
}
merge
.
segmentsClone
=
(
SegmentInfos
)
merge
.
segments
.
clone
(
)
;
for
(
int
i
=
0
;
i
<
end
;
i
++
)
{
SegmentInfo
si
=
merge
.
segmentsClone
.
info
(
i
)
;
if
(
si
.
dir
==
directory
)
deleter
.
incRef
(
si
.
files
(
)
)
;
}
merge
.
increfDone
=
true
;
merge
.
mergeDocStores
=
mergeDocStores
;
merge
.
info
=
new
SegmentInfo
(
newSegmentName
(
)
,
0
,
directory
,
false
,
true
,
docStoreOffset
,
docStoreSegment
,
docStoreIsCompoundFile
,
false
)
;
mergingSegments
.
add
(
merge
.
info
)
;
}
private
synchronized
boolean
doCommitBeforeMergeCFS
(
MergePolicy
.
OneMerge
merge
)
throws
IOException
{
long
freeableBytes
=
0
;
final
int
size
=
merge
.
segments
.
size
(
)
;
for
(
int
i
=
0
;
i
<
size
;
i
++
)
{
final
SegmentInfo
info
=
merge
.
segments
.
info
(
i
)
;
Integer
loc
=
(
Integer
)
rollbackSegments
.
get
(
info
)
;
if
(
loc
!=
null
)
{
final
SegmentInfo
oldInfo
=
rollbackSegmentInfos
.
info
(
loc
.
intValue
(
)
)
;
if
(
oldInfo
.
getUseCompoundFile
(
)
!=
info
.
getUseCompoundFile
(
)
)
freeableBytes
+=
info
.
sizeInBytes
(
)
;
}
}
long
totalBytes
=
0
;
final
int
numSegments
=
segmentInfos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
numSegments
;
i
++
)
totalBytes
+=
segmentInfos
.
info
(
i
)
.
sizeInBytes
(
)
;
if
(
3
*
freeableBytes
>
totalBytes
)
return
true
;
else
return
false
;
}
final
synchronized
void
mergeFinish
(
MergePolicy
.
OneMerge
merge
)
throws
IOException
{
notifyAll
(
)
;
if
(
merge
.
increfDone
)
decrefMergeSegments
(
merge
)
;
assert
merge
.
registerDone
;
final
SegmentInfos
sourceSegments
=
merge
.
segments
;
final
int
end
=
sourceSegments
.
size
(
)
;
for
(
int
i
=
0
;
i
<
end
;
i
++
)
mergingSegments
.
remove
(
sourceSegments
.
info
(
i
)
)
;
mergingSegments
.
remove
(
merge
.
info
)
;
merge
.
registerDone
=
false
;
}
final
private
int
mergeMiddle
(
MergePolicy
.
OneMerge
merge
)
throws
CorruptIndexException
,
IOException
{
merge
.
checkAborted
(
directory
)
;
final
String
mergedName
=
merge
.
info
.
name
;
SegmentMerger
merger
=
null
;
int
mergedDocCount
=
0
;
SegmentInfos
sourceSegments
=
merge
.
segments
;
SegmentInfos
sourceSegmentsClone
=
merge
.
segmentsClone
;
final
int
numSegments
=
sourceSegments
.
size
(
)
;
if
(
infoStream
!=
null
)
message
(
"merging "
+
merge
.
segString
(
directory
)
)
;
merger
=
new
SegmentMerger
(
this
,
mergedName
,
merge
)
;
boolean
success
=
false
;
try
{
int
totDocCount
=
0
;
for
(
int
i
=
0
;
i
<
numSegments
;
i
++
)
{
SegmentInfo
si
=
sourceSegmentsClone
.
info
(
i
)
;
IndexReader
reader
=
SegmentReader
.
get
(
true
,
si
,
MERGE_READ_BUFFER_SIZE
,
merge
.
mergeDocStores
)
;
merger
.
add
(
reader
)
;
totDocCount
+=
reader
.
numDocs
(
)
;
}
if
(
infoStream
!=
null
)
{
message
(
"merge: total "
+
totDocCount
+
" docs"
)
;
}
merge
.
checkAborted
(
directory
)
;
mergedDocCount
=
merge
.
info
.
docCount
=
merger
.
merge
(
merge
.
mergeDocStores
)
;
assert
mergedDocCount
==
totDocCount
;
success
=
true
;
}
finally
{
if
(
merger
!=
null
)
{
merger
.
closeReaders
(
)
;
}
}
if
(
!
commitMerge
(
merge
,
merger
,
mergedDocCount
)
)
return
0
;
if
(
merge
.
useCompoundFile
)
{
if
(
autoCommit
&&
doCommitBeforeMergeCFS
(
merge
)
)
{
final
long
size
;
synchronized
(
this
)
{
size
=
merge
.
info
.
sizeInBytes
(
)
;
}
commit
(
size
)
;
}
success
=
false
;
final
String
compoundFileName
=
mergedName
+
"."
+
IndexFileNames
.
COMPOUND_FILE_EXTENSION
;
try
{
merger
.
createCompoundFile
(
compoundFileName
)
;
success
=
true
;
}
catch
(
IOException
ioe
)
{
synchronized
(
this
)
{
if
(
merge
.
isAborted
(
)
)
{
success
=
true
;
}
else
handleMergeException
(
ioe
,
merge
)
;
}
}
catch
(
Throwable
t
)
{
handleMergeException
(
t
,
merge
)
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception creating compound file during merge"
)
;
synchronized
(
this
)
{
deleter
.
deleteFile
(
compoundFileName
)
;
}
}
}
if
(
merge
.
isAborted
(
)
)
{
if
(
infoStream
!=
null
)
message
(
"abort merge after building CFS"
)
;
deleter
.
deleteFile
(
compoundFileName
)
;
return
0
;
}
synchronized
(
this
)
{
if
(
segmentInfos
.
indexOf
(
merge
.
info
)
==
-
1
||
merge
.
isAborted
(
)
)
{
deleter
.
deleteFile
(
compoundFileName
)
;
}
else
{
merge
.
info
.
setUseCompoundFile
(
true
)
;
checkpoint
(
)
;
}
}
}
if
(
autoCommit
)
{
final
long
size
;
synchronized
(
this
)
{
size
=
merge
.
info
.
sizeInBytes
(
)
;
}
commit
(
size
)
;
}
return
mergedDocCount
;
}
synchronized
void
addMergeException
(
MergePolicy
.
OneMerge
merge
)
{
assert
merge
.
getException
(
)
!=
null
;
if
(
!
mergeExceptions
.
contains
(
merge
)
&&
mergeGen
==
merge
.
mergeGen
)
mergeExceptions
.
add
(
merge
)
;
}
private
final
synchronized
boolean
applyDeletes
(
)
throws
CorruptIndexException
,
IOException
{
assert
testPoint
(
"startApplyDeletes"
)
;
SegmentInfos
rollback
=
(
SegmentInfos
)
segmentInfos
.
clone
(
)
;
boolean
success
=
false
;
boolean
changed
;
try
{
changed
=
docWriter
.
applyDeletes
(
segmentInfos
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
if
(
infoStream
!=
null
)
message
(
"hit exception flushing deletes"
)
;
final
int
size
=
rollback
.
size
(
)
;
for
(
int
i
=
0
;
i
<
size
;
i
++
)
{
final
String
newDelFileName
=
segmentInfos
.
info
(
i
)
.
getDelFileName
(
)
;
final
String
delFileName
=
rollback
.
info
(
i
)
.
getDelFileName
(
)
;
if
(
newDelFileName
!=
null
&&
!
newDelFileName
.
equals
(
delFileName
)
)
deleter
.
deleteFile
(
newDelFileName
)
;
}
segmentInfos
.
clear
(
)
;
segmentInfos
.
addAll
(
rollback
)
;
}
}
if
(
changed
)
checkpoint
(
)
;
return
changed
;
}
final
synchronized
int
getBufferedDeleteTermsSize
(
)
{
return
docWriter
.
getBufferedDeleteTerms
(
)
.
size
(
)
;
}
final
synchronized
int
getNumBufferedDeleteTerms
(
)
{
return
docWriter
.
getNumBufferedDeleteTerms
(
)
;
}
SegmentInfo
newestSegment
(
)
{
return
segmentInfos
.
info
(
segmentInfos
.
size
(
)
-
1
)
;
}
public
synchronized
String
segString
(
)
{
return
segString
(
segmentInfos
)
;
}
private
synchronized
String
segString
(
SegmentInfos
infos
)
{
StringBuffer
buffer
=
new
StringBuffer
(
)
;
final
int
count
=
infos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
count
;
i
++
)
{
if
(
i
>
0
)
{
buffer
.
append
(
' '
)
;
}
final
SegmentInfo
info
=
infos
.
info
(
i
)
;
buffer
.
append
(
info
.
segString
(
directory
)
)
;
if
(
info
.
dir
!=
directory
)
buffer
.
append
(
"**"
)
;
}
return
buffer
.
toString
(
)
;
}
private
HashSet
synced
=
new
HashSet
(
)
;
private
HashSet
syncing
=
new
HashSet
(
)
;
private
boolean
startSync
(
String
fileName
,
Collection
pending
)
{
synchronized
(
synced
)
{
if
(
!
synced
.
contains
(
fileName
)
)
{
if
(
!
syncing
.
contains
(
fileName
)
)
{
syncing
.
add
(
fileName
)
;
return
true
;
}
else
{
pending
.
add
(
fileName
)
;
return
false
;
}
}
else
return
false
;
}
}
private
void
finishSync
(
String
fileName
,
boolean
success
)
{
synchronized
(
synced
)
{
assert
syncing
.
contains
(
fileName
)
;
syncing
.
remove
(
fileName
)
;
if
(
success
)
synced
.
add
(
fileName
)
;
synced
.
notifyAll
(
)
;
}
}
private
boolean
waitForAllSynced
(
Collection
syncing
)
throws
IOException
{
synchronized
(
synced
)
{
Iterator
it
=
syncing
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
final
String
fileName
=
(
String
)
it
.
next
(
)
;
while
(
!
synced
.
contains
(
fileName
)
)
{
if
(
!
syncing
.
contains
(
fileName
)
)
return
false
;
else
try
{
synced
.
wait
(
)
;
}
catch
(
InterruptedException
ie
)
{
continue
;
}
}
}
return
true
;
}
}
private
void
syncPause
(
long
sizeInBytes
)
{
if
(
mergeScheduler
instanceof
ConcurrentMergeScheduler
&&
maxSyncPauseSeconds
>
0
)
{
long
pauseTime
=
(
long
)
(
1000
*
sizeInBytes
/
10
/
1024
/
1024
)
;
final
long
maxPauseTime
=
(
long
)
(
maxSyncPauseSeconds
*
1000
)
;
if
(
pauseTime
>
maxPauseTime
)
pauseTime
=
maxPauseTime
;
final
int
sleepCount
=
(
int
)
(
pauseTime
/
100
)
;
for
(
int
i
=
0
;
i
<
sleepCount
;
i
++
)
{
synchronized
(
this
)
{
if
(
stopMerges
||
closing
)
break
;
}
try
{
Thread
.
sleep
(
100
)
;
}
catch
(
InterruptedException
ie
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
}
}
}
}
private
synchronized
void
doWait
(
)
{
try
{
wait
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
}
}
private
void
startCommit
(
long
sizeInBytes
)
throws
IOException
{
assert
testPoint
(
"startStartCommit"
)
;
if
(
hitOOM
)
return
;
try
{
if
(
infoStream
!=
null
)
message
(
"startCommit(): start sizeInBytes="
+
sizeInBytes
)
;
if
(
sizeInBytes
>
0
)
syncPause
(
sizeInBytes
)
;
SegmentInfos
toSync
=
null
;
final
long
myChangeCount
;
synchronized
(
this
)
{
if
(
sizeInBytes
>
0
&&
stopMerges
)
return
;
blockAddIndexes
(
false
)
;
assert
!
hasExternalSegments
(
)
;
try
{
assert
lastCommitChangeCount
<=
changeCount
;
if
(
changeCount
==
lastCommitChangeCount
)
{
if
(
infoStream
!=
null
)
message
(
"  skip startCommit(): no changes pending"
)
;
return
;
}
if
(
infoStream
!=
null
)
message
(
"startCommit index="
+
segString
(
segmentInfos
)
+
" changeCount="
+
changeCount
)
;
toSync
=
(
SegmentInfos
)
segmentInfos
.
clone
(
)
;
deleter
.
incRef
(
toSync
,
false
)
;
myChangeCount
=
changeCount
;
}
finally
{
resumeAddIndexes
(
)
;
}
}
assert
testPoint
(
"midStartCommit"
)
;
boolean
setPending
=
false
;
try
{
while
(
true
)
{
final
Collection
pending
=
new
ArrayList
(
)
;
for
(
int
i
=
0
;
i
<
toSync
.
size
(
)
;
i
++
)
{
final
SegmentInfo
info
=
toSync
.
info
(
i
)
;
final
List
files
=
info
.
files
(
)
;
for
(
int
j
=
0
;
j
<
files
.
size
(
)
;
j
++
)
{
final
String
fileName
=
(
String
)
files
.
get
(
j
)
;
if
(
startSync
(
fileName
,
pending
)
)
{
boolean
success
=
false
;
try
{
assert
directory
.
fileExists
(
fileName
)
:
"file '"
+
fileName
+
"' does not exist dir="
+
directory
;
message
(
"now sync "
+
fileName
)
;
directory
.
sync
(
fileName
)
;
success
=
true
;
}
finally
{
finishSync
(
fileName
,
success
)
;
}
}
}
}
if
(
waitForAllSynced
(
pending
)
)
break
;
}
assert
testPoint
(
"midStartCommit2"
)
;
synchronized
(
this
)
{
if
(
myChangeCount
>
lastCommitChangeCount
&&
(
pendingCommit
==
null
||
myChangeCount
>
pendingCommitChangeCount
)
)
{
while
(
pendingCommit
!=
null
)
{
message
(
"wait for existing pendingCommit to finish..."
)
;
doWait
(
)
;
}
if
(
segmentInfos
.
getGeneration
(
)
>
toSync
.
getGeneration
(
)
)
toSync
.
updateGeneration
(
segmentInfos
)
;
boolean
success
=
false
;
try
{
try
{
toSync
.
prepareCommit
(
directory
)
;
}
finally
{
segmentInfos
.
updateGeneration
(
toSync
)
;
}
assert
pendingCommit
==
null
;
setPending
=
true
;
pendingCommit
=
toSync
;
pendingCommitChangeCount
=
myChangeCount
;
success
=
true
;
}
finally
{
if
(
!
success
)
message
(
"hit exception committing segments file"
)
;
}
}
else
message
(
"sync superseded by newer infos"
)
;
}
message
(
"done all syncs"
)
;
assert
testPoint
(
"midStartCommitSuccess"
)
;
}
finally
{
synchronized
(
this
)
{
if
(
!
setPending
)
deleter
.
decRef
(
toSync
)
;
}
}
}
catch
(
OutOfMemoryError
oom
)
{
hitOOM
=
true
;
throw
oom
;
}
assert
testPoint
(
"finishStartCommit"
)
;
}
public
static
boolean
isLocked
(
Directory
directory
)
throws
IOException
{
return
directory
.
makeLock
(
WRITE_LOCK_NAME
)
.
isLocked
(
)
;
}
public
static
boolean
isLocked
(
String
directory
)
throws
IOException
{
Directory
dir
=
FSDirectory
.
getDirectory
(
directory
)
;
try
{
return
isLocked
(
dir
)
;
}
finally
{
dir
.
close
(
)
;
}
}
public
static
void
unlock
(
Directory
directory
)
throws
IOException
{
directory
.
makeLock
(
IndexWriter
.
WRITE_LOCK_NAME
)
.
release
(
)
;
}
public
static
final
class
MaxFieldLength
{
private
int
limit
;
private
String
name
;
private
MaxFieldLength
(
String
name
,
int
limit
)
{
this
.
name
=
name
;
this
.
limit
=
limit
;
}
public
MaxFieldLength
(
int
limit
)
{
this
(
"User-specified"
,
limit
)
;
}
public
int
getLimit
(
)
{
return
limit
;
}
public
String
toString
(
)
{
return
name
+
":"
+
limit
;
}
public
static
final
MaxFieldLength
UNLIMITED
=
new
MaxFieldLength
(
"UNLIMITED"
,
Integer
.
MAX_VALUE
)
;
public
static
final
MaxFieldLength
LIMITED
=
new
MaxFieldLength
(
"LIMITED"
,
DEFAULT_MAX_FIELD_LENGTH
)
;
}
boolean
testPoint
(
String
name
)
{
return
true
;
}
}
