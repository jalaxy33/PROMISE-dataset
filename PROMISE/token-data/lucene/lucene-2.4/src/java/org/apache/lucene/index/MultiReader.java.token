package
org
.
apache
.
lucene
.
index
;
import
java
.
io
.
IOException
;
import
java
.
util
.
Collection
;
import
java
.
util
.
Collections
;
import
java
.
util
.
HashMap
;
import
java
.
util
.
Map
;
import
org
.
apache
.
lucene
.
document
.
Document
;
import
org
.
apache
.
lucene
.
document
.
FieldSelector
;
import
org
.
apache
.
lucene
.
index
.
MultiSegmentReader
.
MultiTermDocs
;
import
org
.
apache
.
lucene
.
index
.
MultiSegmentReader
.
MultiTermEnum
;
import
org
.
apache
.
lucene
.
index
.
MultiSegmentReader
.
MultiTermPositions
;
public
class
MultiReader
extends
IndexReader
{
protected
IndexReader
[
]
subReaders
;
private
int
[
]
starts
;
private
boolean
[
]
decrefOnClose
;
private
Map
normsCache
=
new
HashMap
(
)
;
private
int
maxDoc
=
0
;
private
int
numDocs
=
-
1
;
private
boolean
hasDeletions
=
false
;
public
MultiReader
(
IndexReader
[
]
subReaders
)
{
initialize
(
subReaders
,
true
)
;
}
public
MultiReader
(
IndexReader
[
]
subReaders
,
boolean
closeSubReaders
)
{
initialize
(
subReaders
,
closeSubReaders
)
;
}
private
void
initialize
(
IndexReader
[
]
subReaders
,
boolean
closeSubReaders
)
{
this
.
subReaders
=
(
IndexReader
[
]
)
subReaders
.
clone
(
)
;
starts
=
new
int
[
subReaders
.
length
+
1
]
;
decrefOnClose
=
new
boolean
[
subReaders
.
length
]
;
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
{
starts
[
i
]
=
maxDoc
;
maxDoc
+=
subReaders
[
i
]
.
maxDoc
(
)
;
if
(
!
closeSubReaders
)
{
subReaders
[
i
]
.
incRef
(
)
;
decrefOnClose
[
i
]
=
true
;
}
else
{
decrefOnClose
[
i
]
=
false
;
}
if
(
subReaders
[
i
]
.
hasDeletions
(
)
)
hasDeletions
=
true
;
}
starts
[
subReaders
.
length
]
=
maxDoc
;
}
public
IndexReader
reopen
(
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
boolean
reopened
=
false
;
IndexReader
[
]
newSubReaders
=
new
IndexReader
[
subReaders
.
length
]
;
boolean
[
]
newDecrefOnClose
=
new
boolean
[
subReaders
.
length
]
;
boolean
success
=
false
;
try
{
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
{
newSubReaders
[
i
]
=
subReaders
[
i
]
.
reopen
(
)
;
if
(
newSubReaders
[
i
]
!=
subReaders
[
i
]
)
{
reopened
=
true
;
newDecrefOnClose
[
i
]
=
false
;
}
}
if
(
reopened
)
{
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
{
if
(
newSubReaders
[
i
]
==
subReaders
[
i
]
)
{
newSubReaders
[
i
]
.
incRef
(
)
;
newDecrefOnClose
[
i
]
=
true
;
}
}
MultiReader
mr
=
new
MultiReader
(
newSubReaders
)
;
mr
.
decrefOnClose
=
newDecrefOnClose
;
success
=
true
;
return
mr
;
}
else
{
success
=
true
;
return
this
;
}
}
finally
{
if
(
!
success
&&
reopened
)
{
for
(
int
i
=
0
;
i
<
newSubReaders
.
length
;
i
++
)
{
if
(
newSubReaders
[
i
]
!=
null
)
{
try
{
if
(
newDecrefOnClose
[
i
]
)
{
newSubReaders
[
i
]
.
decRef
(
)
;
}
else
{
newSubReaders
[
i
]
.
close
(
)
;
}
}
catch
(
IOException
ignore
)
{
}
}
}
}
}
}
public
TermFreqVector
[
]
getTermFreqVectors
(
int
n
)
throws
IOException
{
ensureOpen
(
)
;
int
i
=
readerIndex
(
n
)
;
return
subReaders
[
i
]
.
getTermFreqVectors
(
n
-
starts
[
i
]
)
;
}
public
TermFreqVector
getTermFreqVector
(
int
n
,
String
field
)
throws
IOException
{
ensureOpen
(
)
;
int
i
=
readerIndex
(
n
)
;
return
subReaders
[
i
]
.
getTermFreqVector
(
n
-
starts
[
i
]
,
field
)
;
}
public
void
getTermFreqVector
(
int
docNumber
,
String
field
,
TermVectorMapper
mapper
)
throws
IOException
{
ensureOpen
(
)
;
int
i
=
readerIndex
(
docNumber
)
;
subReaders
[
i
]
.
getTermFreqVector
(
docNumber
-
starts
[
i
]
,
field
,
mapper
)
;
}
public
void
getTermFreqVector
(
int
docNumber
,
TermVectorMapper
mapper
)
throws
IOException
{
ensureOpen
(
)
;
int
i
=
readerIndex
(
docNumber
)
;
subReaders
[
i
]
.
getTermFreqVector
(
docNumber
-
starts
[
i
]
,
mapper
)
;
}
public
boolean
isOptimized
(
)
{
return
false
;
}
public
synchronized
int
numDocs
(
)
{
if
(
numDocs
==
-
1
)
{
int
n
=
0
;
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
n
+=
subReaders
[
i
]
.
numDocs
(
)
;
numDocs
=
n
;
}
return
numDocs
;
}
public
int
maxDoc
(
)
{
return
maxDoc
;
}
public
Document
document
(
int
n
,
FieldSelector
fieldSelector
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
int
i
=
readerIndex
(
n
)
;
return
subReaders
[
i
]
.
document
(
n
-
starts
[
i
]
,
fieldSelector
)
;
}
public
boolean
isDeleted
(
int
n
)
{
int
i
=
readerIndex
(
n
)
;
return
subReaders
[
i
]
.
isDeleted
(
n
-
starts
[
i
]
)
;
}
public
boolean
hasDeletions
(
)
{
return
hasDeletions
;
}
protected
void
doDelete
(
int
n
)
throws
CorruptIndexException
,
IOException
{
numDocs
=
-
1
;
int
i
=
readerIndex
(
n
)
;
subReaders
[
i
]
.
deleteDocument
(
n
-
starts
[
i
]
)
;
hasDeletions
=
true
;
}
protected
void
doUndeleteAll
(
)
throws
CorruptIndexException
,
IOException
{
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
subReaders
[
i
]
.
undeleteAll
(
)
;
hasDeletions
=
false
;
numDocs
=
-
1
;
}
private
int
readerIndex
(
int
n
)
{
return
MultiSegmentReader
.
readerIndex
(
n
,
this
.
starts
,
this
.
subReaders
.
length
)
;
}
public
boolean
hasNorms
(
String
field
)
throws
IOException
{
ensureOpen
(
)
;
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
{
if
(
subReaders
[
i
]
.
hasNorms
(
field
)
)
return
true
;
}
return
false
;
}
private
byte
[
]
ones
;
private
byte
[
]
fakeNorms
(
)
{
if
(
ones
==
null
)
ones
=
SegmentReader
.
createFakeNorms
(
maxDoc
(
)
)
;
return
ones
;
}
public
synchronized
byte
[
]
norms
(
String
field
)
throws
IOException
{
ensureOpen
(
)
;
byte
[
]
bytes
=
(
byte
[
]
)
normsCache
.
get
(
field
)
;
if
(
bytes
!=
null
)
return
bytes
;
if
(
!
hasNorms
(
field
)
)
return
fakeNorms
(
)
;
bytes
=
new
byte
[
maxDoc
(
)
]
;
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
subReaders
[
i
]
.
norms
(
field
,
bytes
,
starts
[
i
]
)
;
normsCache
.
put
(
field
,
bytes
)
;
return
bytes
;
}
public
synchronized
void
norms
(
String
field
,
byte
[
]
result
,
int
offset
)
throws
IOException
{
ensureOpen
(
)
;
byte
[
]
bytes
=
(
byte
[
]
)
normsCache
.
get
(
field
)
;
if
(
bytes
==
null
&&
!
hasNorms
(
field
)
)
bytes
=
fakeNorms
(
)
;
if
(
bytes
!=
null
)
System
.
arraycopy
(
bytes
,
0
,
result
,
offset
,
maxDoc
(
)
)
;
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
subReaders
[
i
]
.
norms
(
field
,
result
,
offset
+
starts
[
i
]
)
;
}
protected
void
doSetNorm
(
int
n
,
String
field
,
byte
value
)
throws
CorruptIndexException
,
IOException
{
synchronized
(
normsCache
)
{
normsCache
.
remove
(
field
)
;
}
int
i
=
readerIndex
(
n
)
;
subReaders
[
i
]
.
setNorm
(
n
-
starts
[
i
]
,
field
,
value
)
;
}
public
TermEnum
terms
(
)
throws
IOException
{
ensureOpen
(
)
;
return
new
MultiTermEnum
(
subReaders
,
starts
,
null
)
;
}
public
TermEnum
terms
(
Term
term
)
throws
IOException
{
ensureOpen
(
)
;
return
new
MultiTermEnum
(
subReaders
,
starts
,
term
)
;
}
public
int
docFreq
(
Term
t
)
throws
IOException
{
ensureOpen
(
)
;
int
total
=
0
;
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
total
+=
subReaders
[
i
]
.
docFreq
(
t
)
;
return
total
;
}
public
TermDocs
termDocs
(
)
throws
IOException
{
ensureOpen
(
)
;
return
new
MultiTermDocs
(
subReaders
,
starts
)
;
}
public
TermPositions
termPositions
(
)
throws
IOException
{
ensureOpen
(
)
;
return
new
MultiTermPositions
(
subReaders
,
starts
)
;
}
protected
void
doCommit
(
)
throws
IOException
{
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
subReaders
[
i
]
.
commit
(
)
;
}
protected
synchronized
void
doClose
(
)
throws
IOException
{
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
{
if
(
decrefOnClose
[
i
]
)
{
subReaders
[
i
]
.
decRef
(
)
;
}
else
{
subReaders
[
i
]
.
close
(
)
;
}
}
}
public
Collection
getFieldNames
(
IndexReader
.
FieldOption
fieldNames
)
{
ensureOpen
(
)
;
return
MultiSegmentReader
.
getFieldNames
(
fieldNames
,
this
.
subReaders
)
;
}
public
boolean
isCurrent
(
)
throws
CorruptIndexException
,
IOException
{
for
(
int
i
=
0
;
i
<
subReaders
.
length
;
i
++
)
{
if
(
!
subReaders
[
i
]
.
isCurrent
(
)
)
{
return
false
;
}
}
return
true
;
}
public
long
getVersion
(
)
{
throw
new
UnsupportedOperationException
(
"MultiReader does not support this method."
)
;
}
IndexReader
[
]
getSubReaders
(
)
{
return
subReaders
;
}
}
