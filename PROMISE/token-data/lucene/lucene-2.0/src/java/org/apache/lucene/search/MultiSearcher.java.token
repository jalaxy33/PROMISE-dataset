package
org
.
apache
.
lucene
.
search
;
import
java
.
io
.
IOException
;
import
java
.
util
.
HashMap
;
import
java
.
util
.
HashSet
;
import
java
.
util
.
Map
;
import
java
.
util
.
Set
;
import
org
.
apache
.
lucene
.
document
.
Document
;
import
org
.
apache
.
lucene
.
index
.
Term
;
public
class
MultiSearcher
extends
Searcher
{
private
static
class
CachedDfSource
extends
Searcher
{
private
Map
dfMap
;
private
int
maxDoc
;
public
CachedDfSource
(
Map
dfMap
,
int
maxDoc
)
{
this
.
dfMap
=
dfMap
;
this
.
maxDoc
=
maxDoc
;
}
public
int
docFreq
(
Term
term
)
{
int
df
;
try
{
df
=
(
(
Integer
)
dfMap
.
get
(
term
)
)
.
intValue
(
)
;
}
catch
(
NullPointerException
e
)
{
throw
new
IllegalArgumentException
(
"df for term "
+
term
.
text
(
)
+
" not available"
)
;
}
return
df
;
}
public
int
[
]
docFreqs
(
Term
[
]
terms
)
{
int
[
]
result
=
new
int
[
terms
.
length
]
;
for
(
int
i
=
0
;
i
<
terms
.
length
;
i
++
)
{
result
[
i
]
=
docFreq
(
terms
[
i
]
)
;
}
return
result
;
}
public
int
maxDoc
(
)
{
return
maxDoc
;
}
public
Query
rewrite
(
Query
query
)
{
return
query
;
}
public
void
close
(
)
{
throw
new
UnsupportedOperationException
(
)
;
}
public
Document
doc
(
int
i
)
{
throw
new
UnsupportedOperationException
(
)
;
}
public
Explanation
explain
(
Weight
weight
,
int
doc
)
{
throw
new
UnsupportedOperationException
(
)
;
}
public
void
search
(
Weight
weight
,
Filter
filter
,
HitCollector
results
)
{
throw
new
UnsupportedOperationException
(
)
;
}
public
TopDocs
search
(
Weight
weight
,
Filter
filter
,
int
n
)
{
throw
new
UnsupportedOperationException
(
)
;
}
public
TopFieldDocs
search
(
Weight
weight
,
Filter
filter
,
int
n
,
Sort
sort
)
{
throw
new
UnsupportedOperationException
(
)
;
}
}
;
private
Searchable
[
]
searchables
;
private
int
[
]
starts
;
private
int
maxDoc
=
0
;
public
MultiSearcher
(
Searchable
[
]
searchables
)
throws
IOException
{
this
.
searchables
=
searchables
;
starts
=
new
int
[
searchables
.
length
+
1
]
;
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
{
starts
[
i
]
=
maxDoc
;
maxDoc
+=
searchables
[
i
]
.
maxDoc
(
)
;
}
starts
[
searchables
.
length
]
=
maxDoc
;
}
public
Searchable
[
]
getSearchables
(
)
{
return
searchables
;
}
protected
int
[
]
getStarts
(
)
{
return
starts
;
}
public
void
close
(
)
throws
IOException
{
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
searchables
[
i
]
.
close
(
)
;
}
public
int
docFreq
(
Term
term
)
throws
IOException
{
int
docFreq
=
0
;
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
docFreq
+=
searchables
[
i
]
.
docFreq
(
term
)
;
return
docFreq
;
}
public
Document
doc
(
int
n
)
throws
IOException
{
int
i
=
subSearcher
(
n
)
;
return
searchables
[
i
]
.
doc
(
n
-
starts
[
i
]
)
;
}
public
int
subSearcher
(
int
n
)
{
int
lo
=
0
;
int
hi
=
searchables
.
length
-
1
;
while
(
hi
>=
lo
)
{
int
mid
=
(
lo
+
hi
)
>
>
1
;
int
midValue
=
starts
[
mid
]
;
if
(
n
<
midValue
)
hi
=
mid
-
1
;
else
if
(
n
>
midValue
)
lo
=
mid
+
1
;
else
{
while
(
mid
+
1
<
searchables
.
length
&&
starts
[
mid
+
1
]
==
midValue
)
{
mid
++
;
}
return
mid
;
}
}
return
hi
;
}
public
int
subDoc
(
int
n
)
{
return
n
-
starts
[
subSearcher
(
n
)
]
;
}
public
int
maxDoc
(
)
throws
IOException
{
return
maxDoc
;
}
public
TopDocs
search
(
Weight
weight
,
Filter
filter
,
int
nDocs
)
throws
IOException
{
HitQueue
hq
=
new
HitQueue
(
nDocs
)
;
int
totalHits
=
0
;
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
{
TopDocs
docs
=
searchables
[
i
]
.
search
(
weight
,
filter
,
nDocs
)
;
totalHits
+=
docs
.
totalHits
;
ScoreDoc
[
]
scoreDocs
=
docs
.
scoreDocs
;
for
(
int
j
=
0
;
j
<
scoreDocs
.
length
;
j
++
)
{
ScoreDoc
scoreDoc
=
scoreDocs
[
j
]
;
scoreDoc
.
doc
+=
starts
[
i
]
;
if
(
!
hq
.
insert
(
scoreDoc
)
)
break
;
}
}
ScoreDoc
[
]
scoreDocs
=
new
ScoreDoc
[
hq
.
size
(
)
]
;
for
(
int
i
=
hq
.
size
(
)
-
1
;
i
>=
0
;
i
--
)
scoreDocs
[
i
]
=
(
ScoreDoc
)
hq
.
pop
(
)
;
float
maxScore
=
(
totalHits
==
0
)
?
Float
.
NEGATIVE_INFINITY
:
scoreDocs
[
0
]
.
score
;
return
new
TopDocs
(
totalHits
,
scoreDocs
,
maxScore
)
;
}
public
TopFieldDocs
search
(
Weight
weight
,
Filter
filter
,
int
n
,
Sort
sort
)
throws
IOException
{
FieldDocSortedHitQueue
hq
=
null
;
int
totalHits
=
0
;
float
maxScore
=
Float
.
NEGATIVE_INFINITY
;
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
{
TopFieldDocs
docs
=
searchables
[
i
]
.
search
(
weight
,
filter
,
n
,
sort
)
;
if
(
hq
==
null
)
hq
=
new
FieldDocSortedHitQueue
(
docs
.
fields
,
n
)
;
totalHits
+=
docs
.
totalHits
;
maxScore
=
Math
.
max
(
maxScore
,
docs
.
getMaxScore
(
)
)
;
ScoreDoc
[
]
scoreDocs
=
docs
.
scoreDocs
;
for
(
int
j
=
0
;
j
<
scoreDocs
.
length
;
j
++
)
{
ScoreDoc
scoreDoc
=
scoreDocs
[
j
]
;
scoreDoc
.
doc
+=
starts
[
i
]
;
if
(
!
hq
.
insert
(
scoreDoc
)
)
break
;
}
}
ScoreDoc
[
]
scoreDocs
=
new
ScoreDoc
[
hq
.
size
(
)
]
;
for
(
int
i
=
hq
.
size
(
)
-
1
;
i
>=
0
;
i
--
)
scoreDocs
[
i
]
=
(
ScoreDoc
)
hq
.
pop
(
)
;
return
new
TopFieldDocs
(
totalHits
,
scoreDocs
,
hq
.
getFields
(
)
,
maxScore
)
;
}
public
void
search
(
Weight
weight
,
Filter
filter
,
final
HitCollector
results
)
throws
IOException
{
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
{
final
int
start
=
starts
[
i
]
;
searchables
[
i
]
.
search
(
weight
,
filter
,
new
HitCollector
(
)
{
public
void
collect
(
int
doc
,
float
score
)
{
results
.
collect
(
doc
+
start
,
score
)
;
}
}
)
;
}
}
public
Query
rewrite
(
Query
original
)
throws
IOException
{
Query
[
]
queries
=
new
Query
[
searchables
.
length
]
;
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
{
queries
[
i
]
=
searchables
[
i
]
.
rewrite
(
original
)
;
}
return
queries
[
0
]
.
combine
(
queries
)
;
}
public
Explanation
explain
(
Weight
weight
,
int
doc
)
throws
IOException
{
int
i
=
subSearcher
(
doc
)
;
return
searchables
[
i
]
.
explain
(
weight
,
doc
-
starts
[
i
]
)
;
}
protected
Weight
createWeight
(
Query
original
)
throws
IOException
{
Query
rewrittenQuery
=
rewrite
(
original
)
;
Set
terms
=
new
HashSet
(
)
;
rewrittenQuery
.
extractTerms
(
terms
)
;
Term
[
]
allTermsArray
=
new
Term
[
terms
.
size
(
)
]
;
terms
.
toArray
(
allTermsArray
)
;
int
[
]
aggregatedDfs
=
new
int
[
terms
.
size
(
)
]
;
for
(
int
i
=
0
;
i
<
searchables
.
length
;
i
++
)
{
int
[
]
dfs
=
searchables
[
i
]
.
docFreqs
(
allTermsArray
)
;
for
(
int
j
=
0
;
j
<
aggregatedDfs
.
length
;
j
++
)
{
aggregatedDfs
[
j
]
+=
dfs
[
j
]
;
}
}
HashMap
dfMap
=
new
HashMap
(
)
;
for
(
int
i
=
0
;
i
<
allTermsArray
.
length
;
i
++
)
{
dfMap
.
put
(
allTermsArray
[
i
]
,
new
Integer
(
aggregatedDfs
[
i
]
)
)
;
}
int
numDocs
=
maxDoc
(
)
;
CachedDfSource
cacheSim
=
new
CachedDfSource
(
dfMap
,
numDocs
)
;
return
rewrittenQuery
.
weight
(
cacheSim
)
;
}
}
