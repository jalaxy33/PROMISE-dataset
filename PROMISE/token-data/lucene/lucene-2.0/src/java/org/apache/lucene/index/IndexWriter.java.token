package
org
.
apache
.
lucene
.
index
;
import
org
.
apache
.
lucene
.
analysis
.
Analyzer
;
import
org
.
apache
.
lucene
.
document
.
Document
;
import
org
.
apache
.
lucene
.
search
.
Similarity
;
import
org
.
apache
.
lucene
.
store
.
Directory
;
import
org
.
apache
.
lucene
.
store
.
FSDirectory
;
import
org
.
apache
.
lucene
.
store
.
IndexInput
;
import
org
.
apache
.
lucene
.
store
.
IndexOutput
;
import
org
.
apache
.
lucene
.
store
.
Lock
;
import
org
.
apache
.
lucene
.
store
.
RAMDirectory
;
import
java
.
io
.
File
;
import
java
.
io
.
IOException
;
import
java
.
io
.
PrintStream
;
import
java
.
util
.
Vector
;
public
class
IndexWriter
{
public
final
static
long
WRITE_LOCK_TIMEOUT
=
1000
;
private
long
writeLockTimeout
=
WRITE_LOCK_TIMEOUT
;
public
final
static
long
COMMIT_LOCK_TIMEOUT
=
10000
;
private
long
commitLockTimeout
=
COMMIT_LOCK_TIMEOUT
;
public
static
final
String
WRITE_LOCK_NAME
=
"write.lock"
;
public
static
final
String
COMMIT_LOCK_NAME
=
"commit.lock"
;
public
final
static
int
DEFAULT_MERGE_FACTOR
=
10
;
public
final
static
int
DEFAULT_MAX_BUFFERED_DOCS
=
10
;
public
final
static
int
DEFAULT_MAX_MERGE_DOCS
=
Integer
.
MAX_VALUE
;
public
final
static
int
DEFAULT_MAX_FIELD_LENGTH
=
10000
;
public
final
static
int
DEFAULT_TERM_INDEX_INTERVAL
=
128
;
private
Directory
directory
;
private
Analyzer
analyzer
;
private
Similarity
similarity
=
Similarity
.
getDefault
(
)
;
private
SegmentInfos
segmentInfos
=
new
SegmentInfos
(
)
;
private
final
Directory
ramDirectory
=
new
RAMDirectory
(
)
;
private
Lock
writeLock
;
private
int
termIndexInterval
=
DEFAULT_TERM_INDEX_INTERVAL
;
private
boolean
useCompoundFile
=
true
;
private
boolean
closeDir
;
public
boolean
getUseCompoundFile
(
)
{
return
useCompoundFile
;
}
public
void
setUseCompoundFile
(
boolean
value
)
{
useCompoundFile
=
value
;
}
public
void
setSimilarity
(
Similarity
similarity
)
{
this
.
similarity
=
similarity
;
}
public
Similarity
getSimilarity
(
)
{
return
this
.
similarity
;
}
public
void
setTermIndexInterval
(
int
interval
)
{
this
.
termIndexInterval
=
interval
;
}
public
int
getTermIndexInterval
(
)
{
return
termIndexInterval
;
}
public
IndexWriter
(
String
path
,
Analyzer
a
,
boolean
create
)
throws
IOException
{
this
(
FSDirectory
.
getDirectory
(
path
,
create
)
,
a
,
create
,
true
)
;
}
public
IndexWriter
(
File
path
,
Analyzer
a
,
boolean
create
)
throws
IOException
{
this
(
FSDirectory
.
getDirectory
(
path
,
create
)
,
a
,
create
,
true
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
,
boolean
create
)
throws
IOException
{
this
(
d
,
a
,
create
,
false
)
;
}
private
IndexWriter
(
Directory
d
,
Analyzer
a
,
final
boolean
create
,
boolean
closeDir
)
throws
IOException
{
this
.
closeDir
=
closeDir
;
directory
=
d
;
analyzer
=
a
;
Lock
writeLock
=
directory
.
makeLock
(
IndexWriter
.
WRITE_LOCK_NAME
)
;
if
(
!
writeLock
.
obtain
(
writeLockTimeout
)
)
throw
new
IOException
(
"Index locked for write: "
+
writeLock
)
;
this
.
writeLock
=
writeLock
;
synchronized
(
directory
)
{
new
Lock
.
With
(
directory
.
makeLock
(
IndexWriter
.
COMMIT_LOCK_NAME
)
,
commitLockTimeout
)
{
public
Object
doBody
(
)
throws
IOException
{
if
(
create
)
segmentInfos
.
write
(
directory
)
;
else
segmentInfos
.
read
(
directory
)
;
return
null
;
}
}
.
run
(
)
;
}
}
public
void
setMaxMergeDocs
(
int
maxMergeDocs
)
{
this
.
maxMergeDocs
=
maxMergeDocs
;
}
public
int
getMaxMergeDocs
(
)
{
return
maxMergeDocs
;
}
public
void
setMaxFieldLength
(
int
maxFieldLength
)
{
this
.
maxFieldLength
=
maxFieldLength
;
}
public
int
getMaxFieldLength
(
)
{
return
maxFieldLength
;
}
public
void
setMaxBufferedDocs
(
int
maxBufferedDocs
)
{
if
(
maxBufferedDocs
<
2
)
throw
new
IllegalArgumentException
(
"maxBufferedDocs must at least be 2"
)
;
this
.
minMergeDocs
=
maxBufferedDocs
;
}
public
int
getMaxBufferedDocs
(
)
{
return
minMergeDocs
;
}
public
void
setMergeFactor
(
int
mergeFactor
)
{
if
(
mergeFactor
<
2
)
throw
new
IllegalArgumentException
(
"mergeFactor cannot be less than 2"
)
;
this
.
mergeFactor
=
mergeFactor
;
}
public
int
getMergeFactor
(
)
{
return
mergeFactor
;
}
public
void
setInfoStream
(
PrintStream
infoStream
)
{
this
.
infoStream
=
infoStream
;
}
public
PrintStream
getInfoStream
(
)
{
return
infoStream
;
}
public
void
setCommitLockTimeout
(
long
commitLockTimeout
)
{
this
.
commitLockTimeout
=
commitLockTimeout
;
}
public
long
getCommitLockTimeout
(
)
{
return
commitLockTimeout
;
}
public
void
setWriteLockTimeout
(
long
writeLockTimeout
)
{
this
.
writeLockTimeout
=
writeLockTimeout
;
}
public
long
getWriteLockTimeout
(
)
{
return
writeLockTimeout
;
}
public
synchronized
void
close
(
)
throws
IOException
{
flushRamSegments
(
)
;
ramDirectory
.
close
(
)
;
if
(
writeLock
!=
null
)
{
writeLock
.
release
(
)
;
writeLock
=
null
;
}
if
(
closeDir
)
directory
.
close
(
)
;
}
protected
void
finalize
(
)
throws
IOException
{
if
(
writeLock
!=
null
)
{
writeLock
.
release
(
)
;
writeLock
=
null
;
}
}
public
Directory
getDirectory
(
)
{
return
directory
;
}
public
Analyzer
getAnalyzer
(
)
{
return
analyzer
;
}
public
synchronized
int
docCount
(
)
{
int
count
=
0
;
for
(
int
i
=
0
;
i
<
segmentInfos
.
size
(
)
;
i
++
)
{
SegmentInfo
si
=
segmentInfos
.
info
(
i
)
;
count
+=
si
.
docCount
;
}
return
count
;
}
private
int
maxFieldLength
=
DEFAULT_MAX_FIELD_LENGTH
;
public
void
addDocument
(
Document
doc
)
throws
IOException
{
addDocument
(
doc
,
analyzer
)
;
}
public
void
addDocument
(
Document
doc
,
Analyzer
analyzer
)
throws
IOException
{
DocumentWriter
dw
=
new
DocumentWriter
(
ramDirectory
,
analyzer
,
this
)
;
dw
.
setInfoStream
(
infoStream
)
;
String
segmentName
=
newSegmentName
(
)
;
dw
.
addDocument
(
segmentName
,
doc
)
;
synchronized
(
this
)
{
segmentInfos
.
addElement
(
new
SegmentInfo
(
segmentName
,
1
,
ramDirectory
)
)
;
maybeMergeSegments
(
)
;
}
}
final
int
getSegmentsCounter
(
)
{
return
segmentInfos
.
counter
;
}
private
final
synchronized
String
newSegmentName
(
)
{
return
"_"
+
Integer
.
toString
(
segmentInfos
.
counter
++
,
Character
.
MAX_RADIX
)
;
}
private
int
mergeFactor
=
DEFAULT_MERGE_FACTOR
;
private
int
minMergeDocs
=
DEFAULT_MAX_BUFFERED_DOCS
;
private
int
maxMergeDocs
=
DEFAULT_MAX_MERGE_DOCS
;
private
PrintStream
infoStream
=
null
;
public
synchronized
void
optimize
(
)
throws
IOException
{
flushRamSegments
(
)
;
while
(
segmentInfos
.
size
(
)
>
1
||
(
segmentInfos
.
size
(
)
==
1
&&
(
SegmentReader
.
hasDeletions
(
segmentInfos
.
info
(
0
)
)
||
segmentInfos
.
info
(
0
)
.
dir
!=
directory
||
(
useCompoundFile
&&
(
!
SegmentReader
.
usesCompoundFile
(
segmentInfos
.
info
(
0
)
)
||
SegmentReader
.
hasSeparateNorms
(
segmentInfos
.
info
(
0
)
)
)
)
)
)
)
{
int
minSegment
=
segmentInfos
.
size
(
)
-
mergeFactor
;
mergeSegments
(
minSegment
<
0
?
0
:
minSegment
)
;
}
}
public
synchronized
void
addIndexes
(
Directory
[
]
dirs
)
throws
IOException
{
optimize
(
)
;
int
start
=
segmentInfos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
dirs
.
length
;
i
++
)
{
SegmentInfos
sis
=
new
SegmentInfos
(
)
;
sis
.
read
(
dirs
[
i
]
)
;
for
(
int
j
=
0
;
j
<
sis
.
size
(
)
;
j
++
)
{
segmentInfos
.
addElement
(
sis
.
info
(
j
)
)
;
}
}
while
(
segmentInfos
.
size
(
)
>
start
+
mergeFactor
)
{
for
(
int
base
=
start
;
base
<
segmentInfos
.
size
(
)
;
base
++
)
{
int
end
=
Math
.
min
(
segmentInfos
.
size
(
)
,
base
+
mergeFactor
)
;
if
(
end
-
base
>
1
)
mergeSegments
(
base
,
end
)
;
}
}
optimize
(
)
;
}
public
synchronized
void
addIndexes
(
IndexReader
[
]
readers
)
throws
IOException
{
optimize
(
)
;
final
String
mergedName
=
newSegmentName
(
)
;
SegmentMerger
merger
=
new
SegmentMerger
(
this
,
mergedName
)
;
final
Vector
segmentsToDelete
=
new
Vector
(
)
;
IndexReader
sReader
=
null
;
if
(
segmentInfos
.
size
(
)
==
1
)
{
sReader
=
SegmentReader
.
get
(
segmentInfos
.
info
(
0
)
)
;
merger
.
add
(
sReader
)
;
segmentsToDelete
.
addElement
(
sReader
)
;
}
for
(
int
i
=
0
;
i
<
readers
.
length
;
i
++
)
merger
.
add
(
readers
[
i
]
)
;
int
docCount
=
merger
.
merge
(
)
;
segmentInfos
.
setSize
(
0
)
;
segmentInfos
.
addElement
(
new
SegmentInfo
(
mergedName
,
docCount
,
directory
)
)
;
if
(
sReader
!=
null
)
sReader
.
close
(
)
;
synchronized
(
directory
)
{
new
Lock
.
With
(
directory
.
makeLock
(
COMMIT_LOCK_NAME
)
,
commitLockTimeout
)
{
public
Object
doBody
(
)
throws
IOException
{
segmentInfos
.
write
(
directory
)
;
return
null
;
}
}
.
run
(
)
;
}
deleteSegments
(
segmentsToDelete
)
;
if
(
useCompoundFile
)
{
final
Vector
filesToDelete
=
merger
.
createCompoundFile
(
mergedName
+
".tmp"
)
;
synchronized
(
directory
)
{
new
Lock
.
With
(
directory
.
makeLock
(
COMMIT_LOCK_NAME
)
,
commitLockTimeout
)
{
public
Object
doBody
(
)
throws
IOException
{
directory
.
renameFile
(
mergedName
+
".tmp"
,
mergedName
+
".cfs"
)
;
return
null
;
}
}
.
run
(
)
;
}
deleteFiles
(
filesToDelete
)
;
}
}
private
final
void
flushRamSegments
(
)
throws
IOException
{
int
minSegment
=
segmentInfos
.
size
(
)
-
1
;
int
docCount
=
0
;
while
(
minSegment
>=
0
&&
(
segmentInfos
.
info
(
minSegment
)
)
.
dir
==
ramDirectory
)
{
docCount
+=
segmentInfos
.
info
(
minSegment
)
.
docCount
;
minSegment
--
;
}
if
(
minSegment
<
0
||
(
docCount
+
segmentInfos
.
info
(
minSegment
)
.
docCount
)
>
mergeFactor
||
!
(
segmentInfos
.
info
(
segmentInfos
.
size
(
)
-
1
)
.
dir
==
ramDirectory
)
)
minSegment
++
;
if
(
minSegment
>=
segmentInfos
.
size
(
)
)
return
;
mergeSegments
(
minSegment
)
;
}
private
final
void
maybeMergeSegments
(
)
throws
IOException
{
long
targetMergeDocs
=
minMergeDocs
;
while
(
targetMergeDocs
<=
maxMergeDocs
)
{
int
minSegment
=
segmentInfos
.
size
(
)
;
int
mergeDocs
=
0
;
while
(
--
minSegment
>=
0
)
{
SegmentInfo
si
=
segmentInfos
.
info
(
minSegment
)
;
if
(
si
.
docCount
>=
targetMergeDocs
)
break
;
mergeDocs
+=
si
.
docCount
;
}
if
(
mergeDocs
>=
targetMergeDocs
)
mergeSegments
(
minSegment
+
1
)
;
else
break
;
targetMergeDocs
*=
mergeFactor
;
}
}
private
final
void
mergeSegments
(
int
minSegment
)
throws
IOException
{
mergeSegments
(
minSegment
,
segmentInfos
.
size
(
)
)
;
}
private
final
void
mergeSegments
(
int
minSegment
,
int
end
)
throws
IOException
{
final
String
mergedName
=
newSegmentName
(
)
;
if
(
infoStream
!=
null
)
infoStream
.
print
(
"merging segments"
)
;
SegmentMerger
merger
=
new
SegmentMerger
(
this
,
mergedName
)
;
final
Vector
segmentsToDelete
=
new
Vector
(
)
;
for
(
int
i
=
minSegment
;
i
<
end
;
i
++
)
{
SegmentInfo
si
=
segmentInfos
.
info
(
i
)
;
if
(
infoStream
!=
null
)
infoStream
.
print
(
" "
+
si
.
name
+
" ("
+
si
.
docCount
+
" docs)"
)
;
IndexReader
reader
=
SegmentReader
.
get
(
si
)
;
merger
.
add
(
reader
)
;
if
(
(
reader
.
directory
(
)
==
this
.
directory
)
||
(
reader
.
directory
(
)
==
this
.
ramDirectory
)
)
segmentsToDelete
.
addElement
(
reader
)
;
}
int
mergedDocCount
=
merger
.
merge
(
)
;
if
(
infoStream
!=
null
)
{
infoStream
.
println
(
" into "
+
mergedName
+
" ("
+
mergedDocCount
+
" docs)"
)
;
}
for
(
int
i
=
end
-
1
;
i
>
minSegment
;
i
--
)
segmentInfos
.
remove
(
i
)
;
segmentInfos
.
set
(
minSegment
,
new
SegmentInfo
(
mergedName
,
mergedDocCount
,
directory
)
)
;
merger
.
closeReaders
(
)
;
synchronized
(
directory
)
{
new
Lock
.
With
(
directory
.
makeLock
(
COMMIT_LOCK_NAME
)
,
commitLockTimeout
)
{
public
Object
doBody
(
)
throws
IOException
{
segmentInfos
.
write
(
directory
)
;
return
null
;
}
}
.
run
(
)
;
}
deleteSegments
(
segmentsToDelete
)
;
if
(
useCompoundFile
)
{
final
Vector
filesToDelete
=
merger
.
createCompoundFile
(
mergedName
+
".tmp"
)
;
synchronized
(
directory
)
{
new
Lock
.
With
(
directory
.
makeLock
(
COMMIT_LOCK_NAME
)
,
commitLockTimeout
)
{
public
Object
doBody
(
)
throws
IOException
{
directory
.
renameFile
(
mergedName
+
".tmp"
,
mergedName
+
".cfs"
)
;
return
null
;
}
}
.
run
(
)
;
}
deleteFiles
(
filesToDelete
)
;
}
}
private
final
void
deleteSegments
(
Vector
segments
)
throws
IOException
{
Vector
deletable
=
new
Vector
(
)
;
deleteFiles
(
readDeleteableFiles
(
)
,
deletable
)
;
for
(
int
i
=
0
;
i
<
segments
.
size
(
)
;
i
++
)
{
SegmentReader
reader
=
(
SegmentReader
)
segments
.
elementAt
(
i
)
;
if
(
reader
.
directory
(
)
==
this
.
directory
)
deleteFiles
(
reader
.
files
(
)
,
deletable
)
;
else
deleteFiles
(
reader
.
files
(
)
,
reader
.
directory
(
)
)
;
}
writeDeleteableFiles
(
deletable
)
;
}
private
final
void
deleteFiles
(
Vector
files
)
throws
IOException
{
Vector
deletable
=
new
Vector
(
)
;
deleteFiles
(
readDeleteableFiles
(
)
,
deletable
)
;
deleteFiles
(
files
,
deletable
)
;
writeDeleteableFiles
(
deletable
)
;
}
private
final
void
deleteFiles
(
Vector
files
,
Directory
directory
)
throws
IOException
{
for
(
int
i
=
0
;
i
<
files
.
size
(
)
;
i
++
)
directory
.
deleteFile
(
(
String
)
files
.
elementAt
(
i
)
)
;
}
private
final
void
deleteFiles
(
Vector
files
,
Vector
deletable
)
throws
IOException
{
for
(
int
i
=
0
;
i
<
files
.
size
(
)
;
i
++
)
{
String
file
=
(
String
)
files
.
elementAt
(
i
)
;
try
{
directory
.
deleteFile
(
file
)
;
}
catch
(
IOException
e
)
{
if
(
directory
.
fileExists
(
file
)
)
{
if
(
infoStream
!=
null
)
infoStream
.
println
(
e
.
toString
(
)
+
"; Will re-try later."
)
;
deletable
.
addElement
(
file
)
;
}
}
}
}
private
final
Vector
readDeleteableFiles
(
)
throws
IOException
{
Vector
result
=
new
Vector
(
)
;
if
(
!
directory
.
fileExists
(
IndexFileNames
.
DELETABLE
)
)
return
result
;
IndexInput
input
=
directory
.
openInput
(
IndexFileNames
.
DELETABLE
)
;
try
{
for
(
int
i
=
input
.
readInt
(
)
;
i
>
0
;
i
--
)
result
.
addElement
(
input
.
readString
(
)
)
;
}
finally
{
input
.
close
(
)
;
}
return
result
;
}
private
final
void
writeDeleteableFiles
(
Vector
files
)
throws
IOException
{
IndexOutput
output
=
directory
.
createOutput
(
"deleteable.new"
)
;
try
{
output
.
writeInt
(
files
.
size
(
)
)
;
for
(
int
i
=
0
;
i
<
files
.
size
(
)
;
i
++
)
output
.
writeString
(
(
String
)
files
.
elementAt
(
i
)
)
;
}
finally
{
output
.
close
(
)
;
}
directory
.
renameFile
(
"deleteable.new"
,
IndexFileNames
.
DELETABLE
)
;
}
}
