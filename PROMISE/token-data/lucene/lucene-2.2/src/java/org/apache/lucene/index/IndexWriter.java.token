package
org
.
apache
.
lucene
.
index
;
import
org
.
apache
.
lucene
.
analysis
.
Analyzer
;
import
org
.
apache
.
lucene
.
document
.
Document
;
import
org
.
apache
.
lucene
.
search
.
Similarity
;
import
org
.
apache
.
lucene
.
store
.
Directory
;
import
org
.
apache
.
lucene
.
store
.
FSDirectory
;
import
org
.
apache
.
lucene
.
store
.
Lock
;
import
org
.
apache
.
lucene
.
store
.
LockObtainFailedException
;
import
org
.
apache
.
lucene
.
store
.
AlreadyClosedException
;
import
org
.
apache
.
lucene
.
store
.
RAMDirectory
;
import
java
.
io
.
File
;
import
java
.
io
.
IOException
;
import
java
.
io
.
PrintStream
;
import
java
.
util
.
ArrayList
;
import
java
.
util
.
List
;
import
java
.
util
.
HashMap
;
import
java
.
util
.
Iterator
;
import
java
.
util
.
Map
.
Entry
;
public
class
IndexWriter
{
public
static
long
WRITE_LOCK_TIMEOUT
=
1000
;
private
long
writeLockTimeout
=
WRITE_LOCK_TIMEOUT
;
public
static
final
String
WRITE_LOCK_NAME
=
"write.lock"
;
public
final
static
int
DEFAULT_MERGE_FACTOR
=
10
;
public
final
static
int
DEFAULT_MAX_BUFFERED_DOCS
=
10
;
public
final
static
int
DEFAULT_MAX_BUFFERED_DELETE_TERMS
=
1000
;
public
final
static
int
DEFAULT_MAX_MERGE_DOCS
=
Integer
.
MAX_VALUE
;
public
final
static
int
DEFAULT_MAX_FIELD_LENGTH
=
10000
;
public
final
static
int
DEFAULT_TERM_INDEX_INTERVAL
=
128
;
private
final
static
int
MERGE_READ_BUFFER_SIZE
=
4096
;
private
Directory
directory
;
private
Analyzer
analyzer
;
private
Similarity
similarity
=
Similarity
.
getDefault
(
)
;
private
boolean
commitPending
;
private
SegmentInfos
rollbackSegmentInfos
;
private
SegmentInfos
localRollbackSegmentInfos
;
private
boolean
localAutoCommit
;
private
boolean
autoCommit
=
true
;
SegmentInfos
segmentInfos
=
new
SegmentInfos
(
)
;
SegmentInfos
ramSegmentInfos
=
new
SegmentInfos
(
)
;
private
final
RAMDirectory
ramDirectory
=
new
RAMDirectory
(
)
;
private
IndexFileDeleter
deleter
;
private
Lock
writeLock
;
private
int
termIndexInterval
=
DEFAULT_TERM_INDEX_INTERVAL
;
private
int
maxBufferedDeleteTerms
=
DEFAULT_MAX_BUFFERED_DELETE_TERMS
;
private
HashMap
bufferedDeleteTerms
=
new
HashMap
(
)
;
private
int
numBufferedDeleteTerms
=
0
;
private
boolean
useCompoundFile
=
true
;
private
boolean
closeDir
;
private
boolean
closed
;
protected
final
void
ensureOpen
(
)
throws
AlreadyClosedException
{
if
(
closed
)
{
throw
new
AlreadyClosedException
(
"this IndexWriter is closed"
)
;
}
}
public
boolean
getUseCompoundFile
(
)
{
ensureOpen
(
)
;
return
useCompoundFile
;
}
public
void
setUseCompoundFile
(
boolean
value
)
{
ensureOpen
(
)
;
useCompoundFile
=
value
;
}
public
void
setSimilarity
(
Similarity
similarity
)
{
ensureOpen
(
)
;
this
.
similarity
=
similarity
;
}
public
Similarity
getSimilarity
(
)
{
ensureOpen
(
)
;
return
this
.
similarity
;
}
public
void
setTermIndexInterval
(
int
interval
)
{
ensureOpen
(
)
;
this
.
termIndexInterval
=
interval
;
}
public
int
getTermIndexInterval
(
)
{
ensureOpen
(
)
;
return
termIndexInterval
;
}
public
IndexWriter
(
String
path
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
create
,
true
,
null
,
true
)
;
}
public
IndexWriter
(
File
path
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
create
,
true
,
null
,
true
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
null
,
true
)
;
}
public
IndexWriter
(
String
path
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
true
,
null
,
true
)
;
}
public
IndexWriter
(
File
path
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
FSDirectory
.
getDirectory
(
path
)
,
a
,
true
,
null
,
true
)
;
}
public
IndexWriter
(
Directory
d
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
null
,
true
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
null
,
autoCommit
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
,
boolean
create
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
null
,
autoCommit
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
,
IndexDeletionPolicy
deletionPolicy
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
false
,
deletionPolicy
,
autoCommit
)
;
}
public
IndexWriter
(
Directory
d
,
boolean
autoCommit
,
Analyzer
a
,
boolean
create
,
IndexDeletionPolicy
deletionPolicy
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
init
(
d
,
a
,
create
,
false
,
deletionPolicy
,
autoCommit
)
;
}
private
void
init
(
Directory
d
,
Analyzer
a
,
boolean
closeDir
,
IndexDeletionPolicy
deletionPolicy
,
boolean
autoCommit
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
if
(
IndexReader
.
indexExists
(
d
)
)
{
init
(
d
,
a
,
false
,
closeDir
,
deletionPolicy
,
autoCommit
)
;
}
else
{
init
(
d
,
a
,
true
,
closeDir
,
deletionPolicy
,
autoCommit
)
;
}
}
private
void
init
(
Directory
d
,
Analyzer
a
,
final
boolean
create
,
boolean
closeDir
,
IndexDeletionPolicy
deletionPolicy
,
boolean
autoCommit
)
throws
CorruptIndexException
,
LockObtainFailedException
,
IOException
{
this
.
closeDir
=
closeDir
;
directory
=
d
;
analyzer
=
a
;
this
.
infoStream
=
defaultInfoStream
;
if
(
create
)
{
directory
.
clearLock
(
IndexWriter
.
WRITE_LOCK_NAME
)
;
}
Lock
writeLock
=
directory
.
makeLock
(
IndexWriter
.
WRITE_LOCK_NAME
)
;
if
(
!
writeLock
.
obtain
(
writeLockTimeout
)
)
throw
new
LockObtainFailedException
(
"Index locked for write: "
+
writeLock
)
;
this
.
writeLock
=
writeLock
;
try
{
if
(
create
)
{
try
{
segmentInfos
.
read
(
directory
)
;
segmentInfos
.
clear
(
)
;
}
catch
(
IOException
e
)
{
}
segmentInfos
.
write
(
directory
)
;
}
else
{
segmentInfos
.
read
(
directory
)
;
}
this
.
autoCommit
=
autoCommit
;
if
(
!
autoCommit
)
{
rollbackSegmentInfos
=
(
SegmentInfos
)
segmentInfos
.
clone
(
)
;
}
deleter
=
new
IndexFileDeleter
(
directory
,
deletionPolicy
==
null
?
new
KeepOnlyLastCommitDeletionPolicy
(
)
:
deletionPolicy
,
segmentInfos
,
infoStream
)
;
}
catch
(
IOException
e
)
{
this
.
writeLock
.
release
(
)
;
this
.
writeLock
=
null
;
throw
e
;
}
}
public
void
setMaxMergeDocs
(
int
maxMergeDocs
)
{
ensureOpen
(
)
;
this
.
maxMergeDocs
=
maxMergeDocs
;
}
public
int
getMaxMergeDocs
(
)
{
ensureOpen
(
)
;
return
maxMergeDocs
;
}
public
void
setMaxFieldLength
(
int
maxFieldLength
)
{
ensureOpen
(
)
;
this
.
maxFieldLength
=
maxFieldLength
;
}
public
int
getMaxFieldLength
(
)
{
ensureOpen
(
)
;
return
maxFieldLength
;
}
public
void
setMaxBufferedDocs
(
int
maxBufferedDocs
)
{
ensureOpen
(
)
;
if
(
maxBufferedDocs
<
2
)
throw
new
IllegalArgumentException
(
"maxBufferedDocs must at least be 2"
)
;
this
.
minMergeDocs
=
maxBufferedDocs
;
}
public
int
getMaxBufferedDocs
(
)
{
ensureOpen
(
)
;
return
minMergeDocs
;
}
public
void
setMaxBufferedDeleteTerms
(
int
maxBufferedDeleteTerms
)
{
ensureOpen
(
)
;
if
(
maxBufferedDeleteTerms
<
1
)
throw
new
IllegalArgumentException
(
"maxBufferedDeleteTerms must at least be 1"
)
;
this
.
maxBufferedDeleteTerms
=
maxBufferedDeleteTerms
;
}
public
int
getMaxBufferedDeleteTerms
(
)
{
ensureOpen
(
)
;
return
maxBufferedDeleteTerms
;
}
public
void
setMergeFactor
(
int
mergeFactor
)
{
ensureOpen
(
)
;
if
(
mergeFactor
<
2
)
throw
new
IllegalArgumentException
(
"mergeFactor cannot be less than 2"
)
;
this
.
mergeFactor
=
mergeFactor
;
}
public
int
getMergeFactor
(
)
{
ensureOpen
(
)
;
return
mergeFactor
;
}
public
static
void
setDefaultInfoStream
(
PrintStream
infoStream
)
{
IndexWriter
.
defaultInfoStream
=
infoStream
;
}
public
static
PrintStream
getDefaultInfoStream
(
)
{
return
IndexWriter
.
defaultInfoStream
;
}
public
void
setInfoStream
(
PrintStream
infoStream
)
{
ensureOpen
(
)
;
this
.
infoStream
=
infoStream
;
deleter
.
setInfoStream
(
infoStream
)
;
}
public
PrintStream
getInfoStream
(
)
{
ensureOpen
(
)
;
return
infoStream
;
}
public
void
setWriteLockTimeout
(
long
writeLockTimeout
)
{
ensureOpen
(
)
;
this
.
writeLockTimeout
=
writeLockTimeout
;
}
public
long
getWriteLockTimeout
(
)
{
ensureOpen
(
)
;
return
writeLockTimeout
;
}
public
static
void
setDefaultWriteLockTimeout
(
long
writeLockTimeout
)
{
IndexWriter
.
WRITE_LOCK_TIMEOUT
=
writeLockTimeout
;
}
public
static
long
getDefaultWriteLockTimeout
(
)
{
return
IndexWriter
.
WRITE_LOCK_TIMEOUT
;
}
public
synchronized
void
close
(
)
throws
CorruptIndexException
,
IOException
{
if
(
!
closed
)
{
flushRamSegments
(
)
;
if
(
commitPending
)
{
segmentInfos
.
write
(
directory
)
;
deleter
.
checkpoint
(
segmentInfos
,
true
)
;
commitPending
=
false
;
rollbackSegmentInfos
=
null
;
}
ramDirectory
.
close
(
)
;
if
(
writeLock
!=
null
)
{
writeLock
.
release
(
)
;
writeLock
=
null
;
}
closed
=
true
;
if
(
closeDir
)
directory
.
close
(
)
;
}
}
protected
void
finalize
(
)
throws
Throwable
{
try
{
if
(
writeLock
!=
null
)
{
writeLock
.
release
(
)
;
writeLock
=
null
;
}
}
finally
{
super
.
finalize
(
)
;
}
}
public
Directory
getDirectory
(
)
{
ensureOpen
(
)
;
return
directory
;
}
public
Analyzer
getAnalyzer
(
)
{
ensureOpen
(
)
;
return
analyzer
;
}
public
synchronized
int
docCount
(
)
{
ensureOpen
(
)
;
int
count
=
ramSegmentInfos
.
size
(
)
;
for
(
int
i
=
0
;
i
<
segmentInfos
.
size
(
)
;
i
++
)
{
SegmentInfo
si
=
segmentInfos
.
info
(
i
)
;
count
+=
si
.
docCount
;
}
return
count
;
}
private
int
maxFieldLength
=
DEFAULT_MAX_FIELD_LENGTH
;
public
void
addDocument
(
Document
doc
)
throws
CorruptIndexException
,
IOException
{
addDocument
(
doc
,
analyzer
)
;
}
public
void
addDocument
(
Document
doc
,
Analyzer
analyzer
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
SegmentInfo
newSegmentInfo
=
buildSingleDocSegment
(
doc
,
analyzer
)
;
synchronized
(
this
)
{
ramSegmentInfos
.
addElement
(
newSegmentInfo
)
;
maybeFlushRamSegments
(
)
;
}
}
SegmentInfo
buildSingleDocSegment
(
Document
doc
,
Analyzer
analyzer
)
throws
CorruptIndexException
,
IOException
{
DocumentWriter
dw
=
new
DocumentWriter
(
ramDirectory
,
analyzer
,
this
)
;
dw
.
setInfoStream
(
infoStream
)
;
String
segmentName
=
newRamSegmentName
(
)
;
dw
.
addDocument
(
segmentName
,
doc
)
;
SegmentInfo
si
=
new
SegmentInfo
(
segmentName
,
1
,
ramDirectory
,
false
,
false
)
;
si
.
setNumFields
(
dw
.
getNumFields
(
)
)
;
return
si
;
}
public
synchronized
void
deleteDocuments
(
Term
term
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
bufferDeleteTerm
(
term
)
;
maybeFlushRamSegments
(
)
;
}
public
synchronized
void
deleteDocuments
(
Term
[
]
terms
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
for
(
int
i
=
0
;
i
<
terms
.
length
;
i
++
)
{
bufferDeleteTerm
(
terms
[
i
]
)
;
}
maybeFlushRamSegments
(
)
;
}
public
void
updateDocument
(
Term
term
,
Document
doc
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
updateDocument
(
term
,
doc
,
getAnalyzer
(
)
)
;
}
public
void
updateDocument
(
Term
term
,
Document
doc
,
Analyzer
analyzer
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
SegmentInfo
newSegmentInfo
=
buildSingleDocSegment
(
doc
,
analyzer
)
;
synchronized
(
this
)
{
bufferDeleteTerm
(
term
)
;
ramSegmentInfos
.
addElement
(
newSegmentInfo
)
;
maybeFlushRamSegments
(
)
;
}
}
final
synchronized
String
newRamSegmentName
(
)
{
return
"_ram_"
+
Integer
.
toString
(
ramSegmentInfos
.
counter
++
,
Character
.
MAX_RADIX
)
;
}
final
synchronized
int
getSegmentCount
(
)
{
return
segmentInfos
.
size
(
)
;
}
final
synchronized
int
getRamSegmentCount
(
)
{
return
ramSegmentInfos
.
size
(
)
;
}
final
synchronized
int
getDocCount
(
int
i
)
{
if
(
i
>=
0
&&
i
<
segmentInfos
.
size
(
)
)
{
return
segmentInfos
.
info
(
i
)
.
docCount
;
}
else
{
return
-
1
;
}
}
final
synchronized
String
newSegmentName
(
)
{
return
"_"
+
Integer
.
toString
(
segmentInfos
.
counter
++
,
Character
.
MAX_RADIX
)
;
}
private
int
mergeFactor
=
DEFAULT_MERGE_FACTOR
;
private
int
minMergeDocs
=
DEFAULT_MAX_BUFFERED_DOCS
;
private
int
maxMergeDocs
=
DEFAULT_MAX_MERGE_DOCS
;
private
PrintStream
infoStream
=
null
;
private
static
PrintStream
defaultInfoStream
=
null
;
public
synchronized
void
optimize
(
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
flushRamSegments
(
)
;
while
(
segmentInfos
.
size
(
)
>
1
||
(
segmentInfos
.
size
(
)
==
1
&&
(
SegmentReader
.
hasDeletions
(
segmentInfos
.
info
(
0
)
)
||
SegmentReader
.
hasSeparateNorms
(
segmentInfos
.
info
(
0
)
)
||
segmentInfos
.
info
(
0
)
.
dir
!=
directory
||
(
useCompoundFile
&&
(
!
SegmentReader
.
usesCompoundFile
(
segmentInfos
.
info
(
0
)
)
)
)
)
)
)
{
int
minSegment
=
segmentInfos
.
size
(
)
-
mergeFactor
;
mergeSegments
(
segmentInfos
,
minSegment
<
0
?
0
:
minSegment
,
segmentInfos
.
size
(
)
)
;
}
}
private
void
startTransaction
(
)
throws
IOException
{
localRollbackSegmentInfos
=
(
SegmentInfos
)
segmentInfos
.
clone
(
)
;
localAutoCommit
=
autoCommit
;
if
(
localAutoCommit
)
{
flushRamSegments
(
)
;
autoCommit
=
false
;
}
else
deleter
.
incRef
(
segmentInfos
,
false
)
;
}
private
void
rollbackTransaction
(
)
throws
IOException
{
autoCommit
=
localAutoCommit
;
segmentInfos
.
clear
(
)
;
segmentInfos
.
addAll
(
localRollbackSegmentInfos
)
;
localRollbackSegmentInfos
=
null
;
deleter
.
checkpoint
(
segmentInfos
,
false
)
;
if
(
!
autoCommit
)
deleter
.
decRef
(
segmentInfos
)
;
deleter
.
refresh
(
)
;
}
private
void
commitTransaction
(
)
throws
IOException
{
autoCommit
=
localAutoCommit
;
boolean
success
=
false
;
try
{
checkpoint
(
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
rollbackTransaction
(
)
;
}
}
if
(
!
autoCommit
)
deleter
.
decRef
(
localRollbackSegmentInfos
)
;
localRollbackSegmentInfos
=
null
;
deleter
.
checkpoint
(
segmentInfos
,
autoCommit
)
;
}
public
synchronized
void
abort
(
)
throws
IOException
{
ensureOpen
(
)
;
if
(
!
autoCommit
)
{
segmentInfos
.
clear
(
)
;
segmentInfos
.
addAll
(
rollbackSegmentInfos
)
;
deleter
.
checkpoint
(
segmentInfos
,
false
)
;
deleter
.
refresh
(
)
;
ramSegmentInfos
=
new
SegmentInfos
(
)
;
bufferedDeleteTerms
.
clear
(
)
;
numBufferedDeleteTerms
=
0
;
commitPending
=
false
;
close
(
)
;
}
else
{
throw
new
IllegalStateException
(
"abort() can only be called when IndexWriter was opened with autoCommit=false"
)
;
}
}
private
void
checkpoint
(
)
throws
IOException
{
if
(
autoCommit
)
{
segmentInfos
.
write
(
directory
)
;
}
else
{
commitPending
=
true
;
}
}
public
synchronized
void
addIndexes
(
Directory
[
]
dirs
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
optimize
(
)
;
int
start
=
segmentInfos
.
size
(
)
;
boolean
success
=
false
;
startTransaction
(
)
;
try
{
for
(
int
i
=
0
;
i
<
dirs
.
length
;
i
++
)
{
SegmentInfos
sis
=
new
SegmentInfos
(
)
;
sis
.
read
(
dirs
[
i
]
)
;
for
(
int
j
=
0
;
j
<
sis
.
size
(
)
;
j
++
)
{
segmentInfos
.
addElement
(
sis
.
info
(
j
)
)
;
}
}
while
(
segmentInfos
.
size
(
)
>
start
+
mergeFactor
)
{
for
(
int
base
=
start
;
base
<
segmentInfos
.
size
(
)
;
base
++
)
{
int
end
=
Math
.
min
(
segmentInfos
.
size
(
)
,
base
+
mergeFactor
)
;
if
(
end
-
base
>
1
)
{
mergeSegments
(
segmentInfos
,
base
,
end
)
;
}
}
}
success
=
true
;
}
finally
{
if
(
success
)
{
commitTransaction
(
)
;
}
else
{
rollbackTransaction
(
)
;
}
}
optimize
(
)
;
}
public
synchronized
void
addIndexesNoOptimize
(
Directory
[
]
dirs
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
flushRamSegments
(
)
;
int
startUpperBound
=
minMergeDocs
;
boolean
success
=
false
;
startTransaction
(
)
;
try
{
for
(
int
i
=
0
;
i
<
dirs
.
length
;
i
++
)
{
if
(
directory
==
dirs
[
i
]
)
{
throw
new
IllegalArgumentException
(
"Cannot add this index to itself"
)
;
}
SegmentInfos
sis
=
new
SegmentInfos
(
)
;
sis
.
read
(
dirs
[
i
]
)
;
for
(
int
j
=
0
;
j
<
sis
.
size
(
)
;
j
++
)
{
SegmentInfo
info
=
sis
.
info
(
j
)
;
segmentInfos
.
addElement
(
info
)
;
while
(
startUpperBound
<
info
.
docCount
)
{
startUpperBound
*=
mergeFactor
;
if
(
startUpperBound
>
maxMergeDocs
)
{
throw
new
IllegalArgumentException
(
"Upper bound cannot exceed maxMergeDocs"
)
;
}
}
}
}
maybeMergeSegments
(
startUpperBound
)
;
int
segmentCount
=
segmentInfos
.
size
(
)
;
int
numTailSegments
=
0
;
while
(
numTailSegments
<
segmentCount
&&
startUpperBound
>=
segmentInfos
.
info
(
segmentCount
-
1
-
numTailSegments
)
.
docCount
)
{
numTailSegments
++
;
}
if
(
numTailSegments
==
0
)
{
success
=
true
;
return
;
}
if
(
checkNonDecreasingLevels
(
segmentCount
-
numTailSegments
)
)
{
int
numSegmentsToCopy
=
0
;
while
(
numSegmentsToCopy
<
segmentCount
&&
directory
!=
segmentInfos
.
info
(
segmentCount
-
1
-
numSegmentsToCopy
)
.
dir
)
{
numSegmentsToCopy
++
;
}
if
(
numSegmentsToCopy
==
0
)
{
success
=
true
;
return
;
}
for
(
int
i
=
segmentCount
-
numSegmentsToCopy
;
i
<
segmentCount
;
i
++
)
{
mergeSegments
(
segmentInfos
,
i
,
i
+
1
)
;
}
if
(
checkNonDecreasingLevels
(
segmentCount
-
numSegmentsToCopy
)
)
{
success
=
true
;
return
;
}
}
mergeSegments
(
segmentInfos
,
segmentCount
-
numTailSegments
,
segmentCount
)
;
if
(
segmentInfos
.
info
(
segmentInfos
.
size
(
)
-
1
)
.
docCount
>
startUpperBound
)
{
maybeMergeSegments
(
startUpperBound
*
mergeFactor
)
;
}
success
=
true
;
}
finally
{
if
(
success
)
{
commitTransaction
(
)
;
}
else
{
rollbackTransaction
(
)
;
}
}
}
public
synchronized
void
addIndexes
(
IndexReader
[
]
readers
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
optimize
(
)
;
final
String
mergedName
=
newSegmentName
(
)
;
SegmentMerger
merger
=
new
SegmentMerger
(
this
,
mergedName
)
;
SegmentInfo
info
;
IndexReader
sReader
=
null
;
try
{
if
(
segmentInfos
.
size
(
)
==
1
)
{
sReader
=
SegmentReader
.
get
(
segmentInfos
.
info
(
0
)
)
;
merger
.
add
(
sReader
)
;
}
for
(
int
i
=
0
;
i
<
readers
.
length
;
i
++
)
merger
.
add
(
readers
[
i
]
)
;
boolean
success
=
false
;
startTransaction
(
)
;
try
{
int
docCount
=
merger
.
merge
(
)
;
if
(
sReader
!=
null
)
{
sReader
.
close
(
)
;
sReader
=
null
;
}
segmentInfos
.
setSize
(
0
)
;
info
=
new
SegmentInfo
(
mergedName
,
docCount
,
directory
,
false
,
true
)
;
segmentInfos
.
addElement
(
info
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
rollbackTransaction
(
)
;
}
else
{
commitTransaction
(
)
;
}
}
}
finally
{
if
(
sReader
!=
null
)
{
sReader
.
close
(
)
;
}
}
if
(
useCompoundFile
)
{
boolean
success
=
false
;
startTransaction
(
)
;
try
{
merger
.
createCompoundFile
(
mergedName
+
".cfs"
)
;
info
.
setUseCompoundFile
(
true
)
;
}
finally
{
if
(
!
success
)
{
rollbackTransaction
(
)
;
}
else
{
commitTransaction
(
)
;
}
}
}
}
void
doAfterFlush
(
)
throws
IOException
{
}
protected
final
void
maybeFlushRamSegments
(
)
throws
CorruptIndexException
,
IOException
{
if
(
ramSegmentInfos
.
size
(
)
>=
minMergeDocs
||
numBufferedDeleteTerms
>=
maxBufferedDeleteTerms
)
{
flushRamSegments
(
)
;
}
}
private
final
synchronized
void
flushRamSegments
(
)
throws
CorruptIndexException
,
IOException
{
flushRamSegments
(
true
)
;
}
protected
final
synchronized
void
flushRamSegments
(
boolean
triggerMerge
)
throws
CorruptIndexException
,
IOException
{
if
(
ramSegmentInfos
.
size
(
)
>
0
||
bufferedDeleteTerms
.
size
(
)
>
0
)
{
mergeSegments
(
ramSegmentInfos
,
0
,
ramSegmentInfos
.
size
(
)
)
;
if
(
triggerMerge
)
maybeMergeSegments
(
minMergeDocs
)
;
}
}
public
final
synchronized
void
flush
(
)
throws
CorruptIndexException
,
IOException
{
ensureOpen
(
)
;
flushRamSegments
(
)
;
}
public
final
long
ramSizeInBytes
(
)
{
ensureOpen
(
)
;
return
ramDirectory
.
sizeInBytes
(
)
;
}
public
final
synchronized
int
numRamDocs
(
)
{
ensureOpen
(
)
;
return
ramSegmentInfos
.
size
(
)
;
}
private
final
void
maybeMergeSegments
(
int
startUpperBound
)
throws
CorruptIndexException
,
IOException
{
long
lowerBound
=
-
1
;
long
upperBound
=
startUpperBound
;
while
(
upperBound
<
maxMergeDocs
)
{
int
minSegment
=
segmentInfos
.
size
(
)
;
int
maxSegment
=
-
1
;
while
(
--
minSegment
>=
0
)
{
SegmentInfo
si
=
segmentInfos
.
info
(
minSegment
)
;
if
(
maxSegment
==
-
1
&&
si
.
docCount
>
lowerBound
&&
si
.
docCount
<=
upperBound
)
{
maxSegment
=
minSegment
;
}
else
if
(
si
.
docCount
>
upperBound
)
{
break
;
}
}
minSegment
++
;
maxSegment
++
;
int
numSegments
=
maxSegment
-
minSegment
;
if
(
numSegments
<
mergeFactor
)
{
break
;
}
else
{
boolean
exceedsUpperLimit
=
false
;
while
(
numSegments
>=
mergeFactor
)
{
int
docCount
=
mergeSegments
(
segmentInfos
,
minSegment
,
minSegment
+
mergeFactor
)
;
numSegments
-=
mergeFactor
;
if
(
docCount
>
upperBound
)
{
minSegment
++
;
exceedsUpperLimit
=
true
;
}
else
{
numSegments
++
;
}
}
if
(
!
exceedsUpperLimit
)
{
break
;
}
}
lowerBound
=
upperBound
;
upperBound
*=
mergeFactor
;
}
}
private
final
int
mergeSegments
(
SegmentInfos
sourceSegments
,
int
minSegment
,
int
end
)
throws
CorruptIndexException
,
IOException
{
boolean
doMerge
=
end
>
0
;
final
String
mergedName
=
newSegmentName
(
)
;
SegmentMerger
merger
=
null
;
final
List
ramSegmentsToDelete
=
new
ArrayList
(
)
;
SegmentInfo
newSegment
=
null
;
int
mergedDocCount
=
0
;
boolean
anyDeletes
=
(
bufferedDeleteTerms
.
size
(
)
!=
0
)
;
try
{
if
(
doMerge
)
{
if
(
infoStream
!=
null
)
infoStream
.
print
(
"merging segments"
)
;
merger
=
new
SegmentMerger
(
this
,
mergedName
)
;
for
(
int
i
=
minSegment
;
i
<
end
;
i
++
)
{
SegmentInfo
si
=
sourceSegments
.
info
(
i
)
;
if
(
infoStream
!=
null
)
infoStream
.
print
(
" "
+
si
.
name
+
" ("
+
si
.
docCount
+
" docs)"
)
;
IndexReader
reader
=
SegmentReader
.
get
(
si
,
MERGE_READ_BUFFER_SIZE
)
;
merger
.
add
(
reader
)
;
if
(
reader
.
directory
(
)
==
this
.
ramDirectory
)
{
ramSegmentsToDelete
.
add
(
si
)
;
}
}
}
SegmentInfos
rollback
=
null
;
boolean
success
=
false
;
try
{
if
(
doMerge
)
{
mergedDocCount
=
merger
.
merge
(
)
;
if
(
infoStream
!=
null
)
{
infoStream
.
println
(
" into "
+
mergedName
+
" ("
+
mergedDocCount
+
" docs)"
)
;
}
newSegment
=
new
SegmentInfo
(
mergedName
,
mergedDocCount
,
directory
,
false
,
true
)
;
}
if
(
sourceSegments
!=
ramSegmentInfos
||
anyDeletes
)
{
rollback
=
(
SegmentInfos
)
segmentInfos
.
clone
(
)
;
}
if
(
doMerge
)
{
if
(
sourceSegments
==
ramSegmentInfos
)
{
segmentInfos
.
addElement
(
newSegment
)
;
}
else
{
for
(
int
i
=
end
-
1
;
i
>
minSegment
;
i
--
)
sourceSegments
.
remove
(
i
)
;
segmentInfos
.
set
(
minSegment
,
newSegment
)
;
}
}
if
(
sourceSegments
==
ramSegmentInfos
)
{
maybeApplyDeletes
(
doMerge
)
;
doAfterFlush
(
)
;
}
checkpoint
(
)
;
success
=
true
;
}
finally
{
if
(
success
)
{
if
(
sourceSegments
==
ramSegmentInfos
)
{
ramSegmentInfos
.
removeAllElements
(
)
;
}
}
else
{
if
(
sourceSegments
==
ramSegmentInfos
&&
!
anyDeletes
)
{
if
(
newSegment
!=
null
&&
segmentInfos
.
size
(
)
>
0
&&
segmentInfos
.
info
(
segmentInfos
.
size
(
)
-
1
)
==
newSegment
)
{
segmentInfos
.
remove
(
segmentInfos
.
size
(
)
-
1
)
;
}
}
else
if
(
rollback
!=
null
)
{
segmentInfos
.
clear
(
)
;
segmentInfos
.
addAll
(
rollback
)
;
}
deleter
.
refresh
(
)
;
}
}
}
finally
{
if
(
doMerge
)
merger
.
closeReaders
(
)
;
}
deleter
.
deleteDirect
(
ramDirectory
,
ramSegmentsToDelete
)
;
deleter
.
checkpoint
(
segmentInfos
,
autoCommit
)
;
if
(
useCompoundFile
&&
doMerge
)
{
boolean
success
=
false
;
try
{
merger
.
createCompoundFile
(
mergedName
+
".cfs"
)
;
newSegment
.
setUseCompoundFile
(
true
)
;
checkpoint
(
)
;
success
=
true
;
}
finally
{
if
(
!
success
)
{
newSegment
.
setUseCompoundFile
(
false
)
;
deleter
.
refresh
(
)
;
}
}
deleter
.
checkpoint
(
segmentInfos
,
autoCommit
)
;
}
return
mergedDocCount
;
}
private
final
void
maybeApplyDeletes
(
boolean
doMerge
)
throws
CorruptIndexException
,
IOException
{
if
(
bufferedDeleteTerms
.
size
(
)
>
0
)
{
if
(
infoStream
!=
null
)
infoStream
.
println
(
"flush "
+
numBufferedDeleteTerms
+
" buffered deleted terms on "
+
segmentInfos
.
size
(
)
+
" segments."
)
;
if
(
doMerge
)
{
IndexReader
reader
=
null
;
try
{
reader
=
SegmentReader
.
get
(
segmentInfos
.
info
(
segmentInfos
.
size
(
)
-
1
)
)
;
applyDeletesSelectively
(
bufferedDeleteTerms
,
reader
)
;
}
finally
{
if
(
reader
!=
null
)
{
try
{
reader
.
doCommit
(
)
;
}
finally
{
reader
.
doClose
(
)
;
}
}
}
}
int
infosEnd
=
segmentInfos
.
size
(
)
;
if
(
doMerge
)
{
infosEnd
--
;
}
for
(
int
i
=
0
;
i
<
infosEnd
;
i
++
)
{
IndexReader
reader
=
null
;
try
{
reader
=
SegmentReader
.
get
(
segmentInfos
.
info
(
i
)
)
;
applyDeletes
(
bufferedDeleteTerms
,
reader
)
;
}
finally
{
if
(
reader
!=
null
)
{
try
{
reader
.
doCommit
(
)
;
}
finally
{
reader
.
doClose
(
)
;
}
}
}
}
bufferedDeleteTerms
.
clear
(
)
;
numBufferedDeleteTerms
=
0
;
}
}
private
final
boolean
checkNonDecreasingLevels
(
int
start
)
{
int
lowerBound
=
-
1
;
int
upperBound
=
minMergeDocs
;
for
(
int
i
=
segmentInfos
.
size
(
)
-
1
;
i
>=
start
;
i
--
)
{
int
docCount
=
segmentInfos
.
info
(
i
)
.
docCount
;
if
(
docCount
<=
lowerBound
)
{
return
false
;
}
while
(
docCount
>
upperBound
)
{
lowerBound
=
upperBound
;
upperBound
*=
mergeFactor
;
}
}
return
true
;
}
final
synchronized
int
getBufferedDeleteTermsSize
(
)
{
return
bufferedDeleteTerms
.
size
(
)
;
}
final
synchronized
int
getNumBufferedDeleteTerms
(
)
{
return
numBufferedDeleteTerms
;
}
private
static
class
Num
{
private
int
num
;
Num
(
int
num
)
{
this
.
num
=
num
;
}
int
getNum
(
)
{
return
num
;
}
void
setNum
(
int
num
)
{
this
.
num
=
num
;
}
}
private
void
bufferDeleteTerm
(
Term
term
)
{
Num
num
=
(
Num
)
bufferedDeleteTerms
.
get
(
term
)
;
if
(
num
==
null
)
{
bufferedDeleteTerms
.
put
(
term
,
new
Num
(
ramSegmentInfos
.
size
(
)
)
)
;
}
else
{
num
.
setNum
(
ramSegmentInfos
.
size
(
)
)
;
}
numBufferedDeleteTerms
++
;
}
private
final
void
applyDeletesSelectively
(
HashMap
deleteTerms
,
IndexReader
reader
)
throws
CorruptIndexException
,
IOException
{
Iterator
iter
=
deleteTerms
.
entrySet
(
)
.
iterator
(
)
;
while
(
iter
.
hasNext
(
)
)
{
Entry
entry
=
(
Entry
)
iter
.
next
(
)
;
Term
term
=
(
Term
)
entry
.
getKey
(
)
;
TermDocs
docs
=
reader
.
termDocs
(
term
)
;
if
(
docs
!=
null
)
{
int
num
=
(
(
Num
)
entry
.
getValue
(
)
)
.
getNum
(
)
;
try
{
while
(
docs
.
next
(
)
)
{
int
doc
=
docs
.
doc
(
)
;
if
(
doc
>=
num
)
{
break
;
}
reader
.
deleteDocument
(
doc
)
;
}
}
finally
{
docs
.
close
(
)
;
}
}
}
}
private
final
void
applyDeletes
(
HashMap
deleteTerms
,
IndexReader
reader
)
throws
CorruptIndexException
,
IOException
{
Iterator
iter
=
deleteTerms
.
entrySet
(
)
.
iterator
(
)
;
while
(
iter
.
hasNext
(
)
)
{
Entry
entry
=
(
Entry
)
iter
.
next
(
)
;
reader
.
deleteDocuments
(
(
Term
)
entry
.
getKey
(
)
)
;
}
}
}
